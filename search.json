[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this website",
    "section": "",
    "text": "This website contains the lecture slides and lab material for the guest lecture by Jelmer Poelstra (MCIC Wooster, Ohio State University) in the HCS7004 Spring 2024 course “Genome Analytics” taught by Jonathan Fresnedo-Ramirez at OSU.\nThe source code for this website can be found at https://github.com/jelmerp/HCS7004-SP24.\n\n\n\n Back to top"
  },
  {
    "objectID": "lecture.html#what-do-we-mean-by-a-pipeline-or-workflow",
    "href": "lecture.html#what-do-we-mean-by-a-pipeline-or-workflow",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "What do we mean by a “pipeline” or “workflow”?",
    "text": "What do we mean by a “pipeline” or “workflow”?\n\n\n \n  \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n  \n \n \n  \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n  \n \n \n  \n   \n  \n  \n   \n  \n  \n   \n  \n  \n   \n  \n  \n   \n  \n  \n   \n  \n  \n   \n  \n  \n   \n  \n  \n   \n  \n  \n   \n  \n \n \n  \n   \n    snakemake_dag\n    \n    \n     \n     \n      \n       \n      \n     \n     \n      \n      \n      all\n      \n      count\n      \n      \n      \n      \n      map\n      \n      \n      \n      \n      trim\n      smp: smpG\n      \n      \n      \n      \n      map\n      \n      \n      \n      \n      trim\n      smp: smpC\n      \n      \n      \n      \n      map\n      \n      \n      \n      \n      trim\n      smp: smpA\n      \n      \n      \n     \n    \n   \n   \n    \n     \n     trimmedFASTQ\n    \n   \n   \n    \n     \n     BAM\n    \n   \n   \n    \n     \n     counttable\n    \n   \n   \n    \n     \n     \n    \n   \n   \n    \n     \n     \n    \n   \n   \n    \n     \n     \n    \n   \n   \n    \n     \n     rawFASTQ"
  },
  {
    "objectID": "lecture.html#what-do-we-mean-by-a-pipeline-or-workflow-1",
    "href": "lecture.html#what-do-we-mean-by-a-pipeline-or-workflow-1",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "What do we mean by a “pipeline” or “workflow”?",
    "text": "What do we mean by a “pipeline” or “workflow”?\nWe may colloquially refer to any consecutive series of steps in an analysis as a pipeline or workflow.\n\nBut here, I am using these words to mean something that can be executed from start to finish with a single command."
  },
  {
    "objectID": "lecture.html#a-basic-pipeline-script",
    "href": "lecture.html#a-basic-pipeline-script",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "A basic pipeline script",
    "text": "A basic pipeline script\nsamples=(smpA smpC smpG)\n\n# Trim:\nfor sample in ${samples[@]}; do\n    scripts/trim.sh data/\"$sample\".fq &gt; res/\"$sample\"_trim.fq\ndone\n\n# Map:\nfor sample in ${samples[@]}; do\n    scripts/map.sh res/\"$sample\"_trim.fq &gt; res/\"$sample\".bam\ndone\n\n# Count:\nscripts/count.sh ${samples[@]} &gt; res/count_table.txt"
  },
  {
    "objectID": "lecture.html#why-create-a-pipeline",
    "href": "lecture.html#why-create-a-pipeline",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Why create a pipeline?",
    "text": "Why create a pipeline?\nWhat are the advantages of creating such a pipeline, rather than running scripts one-by-one as needed?\n\nRerunning everything is much easier.\nRe-applying the same set of analyses in a different project is much easier.\n\n\n\nIt ensures you are including all necessary steps.\nThe pipeline is a form of documentation of the steps taken.\nIt improves reproducibility, e.g. making it easier for others to repeat your analysis."
  },
  {
    "objectID": "lecture.html#challenges-with-basic-pipeline-scripts",
    "href": "lecture.html#challenges-with-basic-pipeline-scripts",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Challenges with basic pipeline scripts",
    "text": "Challenges with basic pipeline scripts\nHow to rerun parts of the pipeline flexibly? This may be necessary after, e.g.:\n\nSome scripts fail for all or some samples.\nAdding a sample.\nNeeding to modify a script or settings somewhere halfway the pipeline.\n\n\n\nBatch job submissions are critical in genomics but pose problems:\nfor sample in ${samples[@]}; do\n    sbatch scripts/trim.sh data/\"$sample\".fq &gt; res/\"$sample\"_trim.fq\n    sbatch scripts/map.sh res/\"$sample\"_trim.fq &gt; res/\"$sample\".bam\ndone\n\nsbatch scripts/count.sh ${samples[@]} &gt; res/count_table.txt\n\n\n\nWhat is the problem here?\n\nSteps that need output files from prior steps won’t wait for those prior steps!"
  },
  {
    "objectID": "lecture.html#potential-solutions-to-these-challenges",
    "href": "lecture.html#potential-solutions-to-these-challenges",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Potential solutions to these challenges",
    "text": "Potential solutions to these challenges\n\nInformal pipeline\nRather than running the pipeline script as an actual script, just use it as scaffolding and submit interactively jobs one-by-one / group-by-group. You don’t have a true pipeline but that may be OK.\n\n\n\nPush the limits of the Bash and Slurm tool set\nWork with if statements, many arguments to scripts, and Slurm “job dependencies” — but this is very hard to manage for more complex workflows.\n\n\n\nUse a formal workflow management tool."
  },
  {
    "objectID": "lecture.html#the-need-for-specialized-tools",
    "href": "lecture.html#the-need-for-specialized-tools",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "The need for specialized tools",
    "text": "The need for specialized tools\n\nPerkel 2019 - https://www.nature.com/articles/d41586-019-02619-z\n\nTypically, researchers codify workflows using general scripting languages such as Python or Bash. But these often lack the necessary flexibility.\n\n\nWorkflows can involve hundreds to thousands of data files; a pipeline must be able to monitor their progress and exit gracefully if any step fails. And pipelines must be smart enough to work out which tasks need to be re-executed and which do not."
  },
  {
    "objectID": "lecture.html#workflow-management-systems",
    "href": "lecture.html#workflow-management-systems",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Workflow management systems",
    "text": "Workflow management systems\nPipeline/workflow tools, often called “workflow management systems”, provide ways to formally describe and execute pipelines.\n\nSome of the most commonly used (command-line based) options in bioinformatics are:\n\nNextflow\nSnakemake\nCommon Workflow Language (CWL)\nWorkflow Description Language (WDL)"
  },
  {
    "objectID": "lecture.html#advantages-of-formal-workflow-management",
    "href": "lecture.html#advantages-of-formal-workflow-management",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Advantages of formal workflow management",
    "text": "Advantages of formal workflow management\nAutomation\n\nDetect & rerun upon changes in input files and failed steps.\nEasily run for other data sets.\nAutomate Slurm job submissions.\nIntegration with software management.\n\n\n\nFlexibility, portability and scalability by separating generic pipeline nuts-and-bolts and:\n\nRun-specific configuration — samples, directories, settings/parameters.\nThings specific to the run-time environment (laptop vs. cluster vs. cloud)."
  },
  {
    "objectID": "lecture.html#writing-pipelines-vs.-using-them",
    "href": "lecture.html#writing-pipelines-vs.-using-them",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Writing pipelines vs. using them",
    "text": "Writing pipelines vs. using them\nMost workflow tools are small “domain-specific” languages -DSLs-, often a sort of extension of a more general language (Python for Snakemake, Groovy for Nextflow).\n\n\nLearning one of these tool to write your own pipelines is perhaps only worth it if you plan to somewhat commonly work on genomics/bioinformatics projects.\n\nBut there are also publicly available pipelines that you can use."
  },
  {
    "objectID": "lecture.html#nf-core-curates-nextflow-pipelines",
    "href": "lecture.html#nf-core-curates-nextflow-pipelines",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "nf-core curates Nextflow pipelines",
    "text": "nf-core curates Nextflow pipelines\nThe “nf-core” initiative (https://nf-co.re) curates a set of best-practice, flexible, and well-documented pipelines written with Nextflow.\nIt also has some of its own tooling built on top of Nextflow to help contributors create more robust and user-friendly pipelines."
  },
  {
    "objectID": "lecture.html#the-nf-core-paper",
    "href": "lecture.html#the-nf-core-paper",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "The nf-core paper",
    "text": "The nf-core paper\n\n\nhttps://www.nature.com/articles/s41587-020-0439-x"
  },
  {
    "objectID": "lecture.html#the-nextflow-paper",
    "href": "lecture.html#the-nextflow-paper",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "The Nextflow paper",
    "text": "The Nextflow paper\n\n\nhttps://www.nature.com/articles/nbt.3820"
  },
  {
    "objectID": "lecture.html#nf-core-pipelines",
    "href": "lecture.html#nf-core-pipelines",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "nf-core pipelines",
    "text": "nf-core pipelines\nnf-core currently has 58 complete pipelines — these are the four most popular ones:"
  },
  {
    "objectID": "lecture.html#todays-lab-nf-core-rnaseq",
    "href": "lecture.html#todays-lab-nf-core-rnaseq",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Today’s lab: nf-core rnaseq",
    "text": "Today’s lab: nf-core rnaseq\nWe will run the nf-core rnaseq pipeline (https://nf-co.re/rnaseq), a reference-based RNA-seq pipeline."
  },
  {
    "objectID": "lecture.html#notes-on-running-an-nf-core-pipeline",
    "href": "lecture.html#notes-on-running-an-nf-core-pipeline",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Notes on running an nf-core pipeline",
    "text": "Notes on running an nf-core pipeline\nRunning a pipeline like this is a bit different (and more involved) than running a typical piece of bioinformatics software — it is, after all, stitching together many operations. Some considerations:\n\nWe only need to install Nextflow and download the workflow files.\nThe pipeline runs all its constituent tools via Singularity containers (this is most common, and recommended) or via Conda environments.\n\n\n\nThe pipeline will submit Slurm batch jobs for us. It tries to parallelize as much as possible, so there are many separate jobs.\n\n\n\n\nWhen using the Nextflow -resume option, the pipeline will check what needs to be (re)run and what doesn’t: this is quite intricate but generally works well.\n\n\n\n\nNextflow makes a distinction between the final output dir and the “working dir”. In the latter, jobs run and produce outputs, some of which are copied the the output dir."
  },
  {
    "objectID": "lecture.html#rna-seq-to-do-what",
    "href": "lecture.html#rna-seq-to-do-what",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "RNA-seq to do what?",
    "text": "RNA-seq to do what?\nThe nf-core rnaseq pipeline is meant for RNA-seq projects that:\n\nAttempt to sequence only mRNA while avoiding non-coding RNAs (“mRNA-seq”).\nDo not distinguish between RNA from different cell types (“bulk RNA-seq”).\nUse short reads (≤150 bp) that do not cover full transcripts but do uniquely ID genes.\nUse reference genomes (are reference-based) to associate reads with genes.\n\n\n\nDownstream, such projects typically aim to statistically compare expression between groups of samples, and have multiple biological replicates per group.\n\n\n\n\n\n\n\nThat might seem quite specific, but this is by far the most common use RNA-seq use case."
  },
  {
    "objectID": "lecture.html#two-main-parts-to-rna-seq-data-analysis",
    "href": "lecture.html#two-main-parts-to-rna-seq-data-analysis",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Two main parts to RNA-seq data analysis",
    "text": "Two main parts to RNA-seq data analysis\nThere are two main parts to the kind of RNA-seq data analysis I just described:\n\n\nFrom reads to counts\n\n\nGenerating a count table using the reads & the reference genome.\nThis what nf-core rnaseq does, which we will run in today’s lab.\n\n\n\n\n\nCount table analysis\n\n\nExploratory data & differential expression analysis is covered in the self-study lab.\nFunctional enrichment (GO, KEGG) analysis is typically also performed."
  },
  {
    "objectID": "lecture.html#overview-of-the-pipeline-again",
    "href": "lecture.html#overview-of-the-pipeline-again",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Overview of the pipeline again",
    "text": "Overview of the pipeline again\nThe inputs are FASTQ and reference genome files (assembly & annotation), and the outputs include a gene count table and many “QC outputs”."
  },
  {
    "objectID": "lecture.html#the-main-steps",
    "href": "lecture.html#the-main-steps",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "The main steps",
    "text": "The main steps\nRead QC and pre-processing\n\nRead QC (FastQC)\nAdapter and quality trimming (TrimGalore)\nOptional removal of rRNA (SortMeRNA) — off by default, but we will include this\n\n\nAlignment & quantification\n\nAlignment to the reference genome/transcriptome (STAR)\nGene expression quantification (Salmon)\n\n\n\nPost-processing, QC, and reporting\n\nPost-processing alignments: sort, index, mark duplicates (samtools, Picard)\nAlignment/count QC (RSeQC, Qualimap, dupRadar, Preseq, DESeq2)\nCreate a QC/metrics report (MultiQC)"
  },
  {
    "objectID": "lecture.html#pipeline-variants",
    "href": "lecture.html#pipeline-variants",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Pipeline variants",
    "text": "Pipeline variants\nThis pipeline is quite flexible and you can turn several steps off, add optional steps, and change individual options for most tools that the pipeline runs.\n\nOptional removal of contaminants (BBSplit)\nMap to 1 or more additional genomes whose sequences may be present as contamination, and remove reads that map better to contaminant genomes.\n\n\n\n\nAlternative quantification routes\n\nUse RSEM instead of Salmon to quantify.\nSkip STAR and perform direct pseudo-alignment & quantification with Salmon.\n\n\n\n\n\n\nTranscript assembly and quantification (StringTie)\nWhile the pipeline is focused on gene-level quantification, it does produce transcript-level counts as well (this is run by default)."
  },
  {
    "objectID": "lecture.html#from-reads-to-counts-overview",
    "href": "lecture.html#from-reads-to-counts-overview",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "From reads to counts: overview",
    "text": "From reads to counts: overview\nThis part is “bioinformatics-heavy” with large files, a need for lots of computing power, and command-line (Unix shell) programs — it specifically involves:\n\nRead pre-processing\nAligning reads to a reference genome\nQuality control (QC) of both the reads and the alignments\nQuantifying expression levels"
  },
  {
    "objectID": "lecture.html#reads-to-counts-read-pre-processing",
    "href": "lecture.html#reads-to-counts-read-pre-processing",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Reads to counts: read pre-processing",
    "text": "Reads to counts: read pre-processing\nRead pre-processing includes the following steps:\n\n\nChecking the quantity and quality of your reads\n\nDoes not change your data, but helps decide next steps / sample exclusion\nAlso useful to check for contamination, library complexity, and adapter content\n\n\n\n\n\nRemoving unwanted sequences\n\nAdapters, low-quality bases, and very short reads\nrRNA-derived reads (optional)\nContaminant sequences (optional)"
  },
  {
    "objectID": "lecture.html#reads-to-counts-alignment-to-a-reference-genome",
    "href": "lecture.html#reads-to-counts-alignment-to-a-reference-genome",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Reads to counts: alignment to a reference genome",
    "text": "Reads to counts: alignment to a reference genome\nThe alignment of reads to a reference genome needs to be “splice-aware”.\n\n\nBerge et al. 2019"
  },
  {
    "objectID": "lecture.html#reads-to-counts-alignment-to-a-reference-genome-1",
    "href": "lecture.html#reads-to-counts-alignment-to-a-reference-genome-1",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Reads to counts: alignment to a reference genome",
    "text": "Reads to counts: alignment to a reference genome\nAlternatively, you can align to the transcriptome (i.e., all mature transcripts):\n\nBerge et al. 2019"
  },
  {
    "objectID": "lecture.html#reads-to-counts-alignment-qc",
    "href": "lecture.html#reads-to-counts-alignment-qc",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Reads to counts: alignment QC",
    "text": "Reads to counts: alignment QC\nHere are some examples of the kinds of things to pay attention to:\n\nAlignment rates\nWhat percentage of reads was successfully aligned? (Should be &gt;80%)\n\n\n\n\nAlignment targets\nWhat percentages of aligned reads mapped to exons vs. introns vs. intergenic regions?\n\n\n\n\n\nWhat might cause high intronic mapping rates?\n\nAn abundance of pre-mRNA versus mature-mRNA.\n\n\n\n\n\nWhat might cause high intergenic mapping rates?\n\nDNA contamination or poor genome assembly/annotation quality"
  },
  {
    "objectID": "lecture.html#reads-to-counts-quantification",
    "href": "lecture.html#reads-to-counts-quantification",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Reads to counts: quantification",
    "text": "Reads to counts: quantification\nAt heart, a simple counting exercise once you have the alignments in hand.\nBut made more complicated by sequencing biases and multi-mapping reads.\n\n\nCurrent best-performing tools (e.g. Salmon) do transcript-level quantification — even though this is typically followed by gene-level aggregation prior to downstream analysis.\n\n\n\n\n\n\n\n\n\n\nFast-moving field\n\n\nSeveral very commonly used tools like FeatureCounts (&gt;15k citations) and HTSeq (&lt;18k citations) have become disfavored in the past couple of years, as they e.g. don’t count multi-mapping reads at all."
  },
  {
    "objectID": "lecture.html#count-table-analysis",
    "href": "lecture.html#count-table-analysis",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Count table analysis",
    "text": "Count table analysis\nThe second part of RNA-seq data analysis involves analyzing the count table.\nIn contrast to the first part, this can be done on a laptop and instead is heavier on statistics, data visualization and biological interpretation.\n\nIt is typically done with the R language, and common steps include:\n\nPrincipal Component Analysis (PCA)\nAssessing overall sample clustering patterns\nDifferential Expression (DE) analysis\nFinding genes that differ in expression level between sample groups (DEGs)\nFunctional enrichment analysis\nSee whether certain gene function groupings are over-represented among DEGs"
  },
  {
    "objectID": "lecture.html#minimal-nextflow-pipeline-example",
    "href": "lecture.html#minimal-nextflow-pipeline-example",
    "title": "Pipelines, nf-core,and nf-core rnaseq—-",
    "section": "Minimal Nextflow pipeline example",
    "text": "Minimal Nextflow pipeline example\nparams.greeting = 'Hello world!' \ngreeting_ch = Channel.of(params.greeting) \n\nprocess SPLITLETTERS { \n    input: \n    val x \n\n    output: \n    path 'chunk_*' \n\n    script: \n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n} \n\nprocess CONVERTTOUPPER { \n    input: \n    path y \n\n    output: \n    stdout \n\n    script: \n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n} \n\nworkflow { \n    letters_ch = SPLITLETTERS(greeting_ch) \n    results_ch = CONVERTTOUPPER(letters_ch.flatten()) \n    results_ch.view { it } \n}"
  },
  {
    "objectID": "lab1.html#introduction",
    "href": "lab1.html#introduction",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn this lab, we will run the nf-core rnaseq pipeline that I discussed in the lecture. Using raw RNA-seq reads in FASTQ files and reference genomes files as inputs, this pipeline will generate a gene count table as its most important output. That gene count table can then be analyzed to examine, for example, differential expression, which is the topic of the self-study lab.\n\n\n\nAn overview of the steps in the nf-core rnaseq pipeline.\n\n\n\nWe will work with the data set from the paper “Two avian Plasmodium species trigger different transcriptional responses on their vector Culex pipiens”, published last year in Molecular Ecology (link):\n\n\n\n\n\nThis paper uses RNA-seq data to study gene expression in Culex pipiens mosquitoes infected with malaria-causing Plasmodium protozoans — specifically, it compares mosquito according to:\n\nInfection status: Plasmodium cathemerium vs. P. relictum vs. control\nTime after infection: 24 h vs. 10 days vs. 21 days"
  },
  {
    "objectID": "lab1.html#getting-started-with-vs-code",
    "href": "lab1.html#getting-started-with-vs-code",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "2 Getting started with VS Code",
    "text": "2 Getting started with VS Code\nWe will use the VS Code text editor to write a script to run the nf-core rnaseq pipeline. To emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. The RStudio program is another good example of an IDE. Just like RStudio is an IDE for R, VS Code will be our IDE for shell code today.\n\n2.1 Starting VS Code at OSC\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and near the bottom, click Code Server.\nVS Code runs on a compute node so we have to fill out a form to make a reservation for one:\n\nThe OSC “Project” that we want to bill for the compute node usage: PAS2658.\nThe “Number of hours” we want to make a reservation for: 2.\nThe “Working Directory” for the program: your personal folder in /fs/scratch/PAS2658 (e.g. /fs/scratch/PAS2658/jelmer).\nThe “Codeserver Version”: 4.8 (most recent).\nClick Launch.\n\nFirst, your job will be “Queued” — that is, waiting for the job scheduler to allocate compute node resources to it.\nYour job is typically granted resources within a few seconds (the card will then say “Starting”), and should be ready for usage (“Running”) in another couple of seconds. Once the job is running click on the blue Connect to VS Code button to open VS Code — it will open in a new browser tab.\nWhen VS Code opens, you may get these two pop-ups (and possibly some others) — click “Yes” (and check the box) and “Don’t Show Again”, respectively:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’ll also get a Welcome/Get Started page — you don’t have to go through steps that may be suggested there.\n\n\n\n\n2.2 The VS Code User Interface\n\n\nClick to see an annotated screenshot\n\n\n\n\nSide bars\nThe narrow side bar on the far left has:\n\nA  (“hamburger menu”), which has menu items like File that you often find in a top bar.\nA  (cog wheel icon) in the bottom, through which you can mainly access settings.\nIcons to toggle between options for what to show in the wide side bar, e.g. a File Explorer (the default option).\n\n\n\n\nTerminal\nOpen a terminal with a Unix shell: click      =&gt; Terminal =&gt; New Terminal. In the terminal, create a dir for this lab, e.g.:\n# You should be in your personal dir in /fs/scratch/PAS2658\npwd\n/fs/scratch/PAS2658/jelmer\nmkdir -p Lab9 \ncd Lab9\nmkdir scripts run software\n\n\n\nEditor pane and Welcome document\nThe main part of the VS Code window is the editor pane. Here, you can open text files and images. Create two new files:\ntouch run/run.sh scripts/nfc-rnaseq.sh\nThen open the run.sh file in the editor – hold Ctrl/Cmd and click on the path in the command you just issued:\n\n\n\n\n\n\n\n\n\n\n\nOr create and open files using the menus (Click to expand)\n\n\n\n\n\n\nOpen a new file: Click the hamburger menu , then File &gt; New File.\nSave the file (Ctrl/⌘+S), inside one of the dirs you just created: Lab9/run/run.sh.\nRepeat steps 1 and 2 to create a file Lab9/scripts/nfc-rnaseq.sh.\nFind the run.sh file in the File Explorer in the left side bar, and click on it to open.\n\n\n\n\n\n\n\n\n\n\nSome VS Code tips and tricks (Click to expand)\n\n\n\n\n\n\nA folder as a starting point\nConveniently, VS Code takes a specific directory as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\nThis is why your terminal was “already” located in /fs/scratch/PAS2658/&lt;your-name&gt;.\n(If you need to switch folders, click      &gt;   File   &gt;   Open Folder.)\nResizing panes\nYou can resize panes (terminal, editor, side bar) by hovering your cursor over the borders and then dragging.\nHide the side bars\nIf you want to save some screen space while coding along in class, you may want to occasionally hide the side bars:\n\nIn  &gt; View &gt; Appearance you can toggle both the Activity Bar (narrow side bar) and the Primary Side Bar (wide side bar).\nOr use keyboard shortcuts:\n\nCtrl/⌘+B for the primary/wide side bar\nCtrl+Shift+B for the activity/narrow side bar\n\n\nThe Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for. To access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P). To use it, start typing something to look for an option.\nKeyboard shortcuts\nFor a single-page PDF overview of keyboard shortcuts for your operating system:    =&gt;   Help   =&gt;   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.)\nInstall the Shellcheck extension\nClick the gear icon  and then Extensions, and search for and then install the shellcheck (by simonwong) extension, which will check your shell scripts for errors, and is extremely useful."
  },
  {
    "objectID": "lab1.html#setting-up",
    "href": "lab1.html#setting-up",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "3 Setting up",
    "text": "3 Setting up\n\n3.1 Getting your own copy of the data\nAs mentioned above, we will use the RNA-seq data from Garrigos et al. 2023. However, to keep things manageable for a lab like this, I have subset the data set we’ll be working with: I omitted the 21-day samples and only kept 500,000 reads per FASTQ file. All in all, our set of files consists of:\n\n44 paired-end Illumina RNA-seq FASTQ files for 22 samples.\nCulex pipiens reference genome files from NCBI: assembly in FASTA format and annotation in GTF format.\nA metadata file in TSV format matching sample IDs with treatment & time point info.\nA README file describing the data set.\n\nGo ahead and get yourself a copy of the data with cp command:\n# (Using the -r option for recursive copying, and -v to print what it's doing)\ncp -rv /fs/scratch/PAS2658/jelmer/share/* .\n‘/fs/scratch/PAS2658/jelmer/share/data’ -&gt; ‘./data’\n‘/fs/scratch/PAS2658/jelmer/share/data/meta’ -&gt; ‘./data/meta’\n‘/fs/scratch/PAS2658/jelmer/share/data/meta/metadata.tsv’ -&gt; ‘./data/meta/metadata.tsv’\n‘/fs/scratch/PAS2658/jelmer/share/data/ref’ -&gt; ‘./data/ref’\n‘/fs/scratch/PAS2658/jelmer/share/data/ref/GCF_016801865.2.gtf’ -&gt; ‘./data/ref/GCF_016801865.2.gtf’\n‘/fs/scratch/PAS2658/jelmer/share/data/ref/GCF_016801865.2.fna’ -&gt; ‘./data/ref/GCF_016801865.2.fna’\n‘/fs/scratch/PAS2658/jelmer/share/data/fastq’ -&gt; ‘./data/fastq’\n‘/fs/scratch/PAS2658/jelmer/share/data/fastq/ERR10802868_R2.fastq.gz’ -&gt; ‘./data/fastq/ERR10802868_R2.fastq.gz’\n‘/fs/scratch/PAS2658/jelmer/share/data/fastq/ERR10802863_R1.fastq.gz’ -&gt; ‘./data/fastq/ERR10802863_R1.fastq.gz’\n‘/fs/scratch/PAS2658/jelmer/share/data/fastq/ERR10802886_R2.fastq.gz’ -&gt; ‘./data/fastq/ERR10802886_R2.fastq.gz’\n# [...output truncated...]\nUse the tree command to get a nice overview of the files you copied:\n# '-C' will add colors to the output (not visible in the output below)\ntree -C data\ndata\n├── fastq\n│   ├── ERR10802863_R1.fastq.gz\n│   ├── ERR10802863_R2.fastq.gz\n│   ├── ERR10802864_R1.fastq.gz\n│   ├── ERR10802864_R2.fastq.gz\n│   ├── ERR10802865_R1.fastq.gz\n│   ├── ERR10802865_R2.fastq.gz\n    ├── [...truncated...]\n├── meta\n│   └── metadata.tsv\n├── README.md\n└── ref\n    ├── GCF_016801865.2.fna\n    └── GCF_016801865.2.gtf\n\n3 directories, 48 files\nWe’ll take a look at some of the files:\n\nThe metadata file:\ncat data/meta/metadata.tsv\nsample_id       time    treatment\nERR10802882     10dpi   cathemerium\nERR10802875     10dpi   cathemerium\nERR10802879     10dpi   cathemerium\nERR10802883     10dpi   cathemerium\nERR10802878     10dpi   control\nERR10802884     10dpi   control\nERR10802877     10dpi   control\nERR10802881     10dpi   control\nERR10802876     10dpi   relictum\nERR10802880     10dpi   relictum\nERR10802885     10dpi   relictum\nERR10802886     10dpi   relictum\nERR10802864     24hpi   cathemerium\nERR10802867     24hpi   cathemerium\nERR10802870     24hpi   cathemerium\nERR10802866     24hpi   control\nERR10802869     24hpi   control\nERR10802863     24hpi   control\nERR10802871     24hpi   relictum\nERR10802874     24hpi   relictum\nERR10802865     24hpi   relictum\nERR10802868     24hpi   relictum\nThe FASTQ files:\nls -lh data/fastq\ntotal 941M\n-rw-r--r-- 1 jelmer PAS2658 21M Mar 23 12:40 ERR10802863_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802863_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 21M Mar 23 12:40 ERR10802864_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802864_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802865_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802865_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 21M Mar 23 12:40 ERR10802866_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802866_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802867_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802867_R2.fastq.gz\n# [...output truncated...]\n\n\n\n\n3.2 How we’ll run the pipeline\nAs discussed in the lecture, the entire nf-core rnaseq pipeline can be run with a single command. That said, before we can do so, we’ll need to a bunch of prep, such as:\n\nActivating the software environment and downloading the pipeline files.\nDefining the pipeline’s inputs and outputs, which includes creating a “sample sheet”.\nCreating a small “config file” to run Nextflow pipelines at OSC.\n\nWe need the latter configuration because the pipeline will submit Slurm batch jobs for us for each step of the pipeline. And in most steps, programs are run independently for each sample, so the pipeline will submit a separate job for each sample for these steps — therefore, we’ll have many jobs altogether (typically 100s).\nThe main Nextflow process does not need much computing power (a single core with the default 4 GB of RAM will be sufficient) and even though our VS Code shell already runs on a compute and not a login node, we are still better off submitting the main process as a batch job as well, because:\n\nThis process can run for hours and we don’t want to risk it disconnecting.\nWe want to store all the standard output about pipeline progress and so on to a file — this will automatically end up in a Slurm log file if we submit it as a batch job.\n\n\n\n\n3.3 Conceptual overview of our script setup\nWe will be working with two scripts in this lab, both of which you already created an empty file for:\n\nA “runner” script that you can also think of as a digital lab notebook, containing commands that we run interactively.\nA script that we will submit as a Slurm batch job with sbatch, containing code to run the nf-core nextflow pipeline.\n\nTo give you an idea of what this will look like — the runner script will include code like this, which will submit the job script:\n# [Don't run or copy this]\nsbatch scripts/nfc_rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\"\nThe variables above (\"$samplesheet\" etc.) are the inputs and outputs of the pipeline, which we will have defined elsewhere in the runner script. Inside the job script, we will then use these variables to run the pipeline in a specific way.\n\n\n\n3.4 Activating the Conda environment\nTo save some time, you won’t do your own Conda installation of Nextflow or nf-core tools — I’ve installed both in an environment you can activate as follows:\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n\n# First load OSC's (mini)Conda module\nmodule load miniconda3\n# Then activate the Nextflow conda environment \nsource activate /fs/ess/PAS0471/jelmer/conda/nextflow\nCheck that Nextflow and nf-core tools can be run by printing the versions:\n# [Run this code directly in the terminal]\nnextflow -v\nnextflow version 23.10.1.5891\n# [Run this code directly in the terminal]\nnf-core --version\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.13.1 - https://nf-co.re\n\nnf-core, version 2.13.1\n\n\n\n3.5 Downloading the nf-core rnaseq pipeline\nWe’ll use the nf-core download command to download the rnaseq pipeline’s files.\nFirst, we need to set the environment variable NXF_SINGULARITY_CACHEDIR to tell Nextflow where to store the Singularity containers for all the tools the pipeline runs1. We will use a dir of mine that already has all containers, to save some downloading time2:\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n\n# Create an environment variable for the container dir\nexport NXF_SINGULARITY_CACHEDIR=/fs/ess/PAS0471/containers\nNext, we’ll run the nf-core download command to download the currently latest version (3.14.0) of the rnaseq pipeline to software/rnaseq, and the associated container files to the previously specified dir:\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n\n# Download the nf-core rnaseq pipeline files\nnf-core download rnaseq \\\n    --revision 3.14.0 \\\n    --outdir software/nfc-rnaseq \\\n    --compress none \\\n    --container-system singularity \\\n    --container-cache-utilisation amend \\\n    --download-configuration\n\n\n\n\n\n\n\n\n\n\n\nExplanation of all options given to nf-core download (Click to expand)\n\n\n\n\n\n\n--revision: The version of the rnaseq pipeline.\n--outdir: The dir to save the pipeline definition files.\n--compress: Whether to compress the pipeline files — we chose not to.\n--container-system: The type of containers to download. This should always be singularity at OSC, because that’s the only supported type.\n--container-cache-utilisation: This is a little technical and not terribly interesting, but we used amend, which will make it check our $NXF_SINGULARITY_CACHEDIR dir for existing containers, and simply download any that aren’t already found there.\n--download-configuration: This will download some configuration files that we will actually not use, but if you don’t provide this option, it will ask you about it when you run the command.\n\nAlso, don’t worry about the following warning, this doesn’t impact the downloading:\n\nWARNING Could not find GitHub authentication token. Some API requests may fail.\n\n\n\n\n\nLet’s take a quick peek at the dirs and files we just downloaded:\n# [Run this code directly in the terminal]\nls software/nfc-rnaseq\n3_14_0  configs\n# [Run this code directly in the terminal]\nls software/nfc-rnaseq/3_14_0\nassets        CODE_OF_CONDUCT.md  LICENSE       nextflow.config       subworkflows\nbin           conf                main.nf       nextflow_schema.json  tower.yml\nCHANGELOG.md  docs                modules       pyproject.toml        workflows\nCITATIONS.md  lib                 modules.json  README.md\nThe dir and file structure here is unfortunately quite complicated, as are the individual pipeline definition files, so we won’t go into further detail about that here."
  },
  {
    "objectID": "lab1.html#writing-a-shell-script-to-run-the-pipeline",
    "href": "lab1.html#writing-a-shell-script-to-run-the-pipeline",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "4 Writing a shell script to run the pipeline",
    "text": "4 Writing a shell script to run the pipeline\nIn this section, we’ll go through the components of the scripts/nfc-rnaseq.sh script that we’ll later submit as a Slurm batch job. The most important part of this script is the nextflow command that will actually run the pipeline.\n\n4.1 Building our nextflow run command\nTo run the pipeline, we use the command nextflow run, followed by the path to the dir that we just downloaded:\n# [Partial shell script code, don't copy or run]\nnextflow run software/nfc-rnaseq/3_14_0\nAfter that, there are several required options (see the pipeline’s documentation), which represent the input and output files/dirs for the pipeline:\n\n--input: The path to a “sample sheet” with the paths to FASTQ files (more on that below).\n--fasta: The path to a reference genome assembly FASTA file — we’ll use the FASTA file we have in data/ref.\n--gtf: The path to a reference genome annotation file3 — we’ll use the GTF file we have in data/ref.\n--outdir: The path to the desired output dir for the final pipeline output — this can be whatever we like.\n\nAs discussed in the lecture, this pipeline has different options for e.g. alignment and quantification. We will stick close to the defaults, which includes alignment with STAR and quantification with Salmon, with one exception: we want to remove reads from ribosomal RNA (this step is skipped by default).\n\n Exercise: Finding the option to remove rRNA\nTake a look at the “Parameters” tab on the pipeline’s documentation website:\n\nBrowse through the options for a bit to get a feel for the extent to which you can customize the pipeline.\nTry to find the option to turn on removal of rRNA with SortMeRNA.\n\n\n\nClick for the solution\n\nThe option we want is --remove_ribo_rna.\n\n\n\nWe’ll also use several general Nextflow options (note the single dash - notation; pipeline-specific options have --):\n\n-profile: A so-called “profile” — should be singularity when running the pipeline with Singularity containers.\nwork-dir: The dir in which all the pipeline’s jobs/processes will run.\n-ansi-log false: Change Nextflow’s progress “logging” type to a format that works with Slurm log files4.\n-resume: Resume the pipeline where it “needs to” (e.g., where it left off) instead of always starting over.\n\n\n\n\n\n\n\nMore on -work-dir and -resume (Click to expand)\n\n\n\n\n\nwork-dir:\nThe pipeline’s final outputs will go to the --outdir we talked about earlier. But all jobs/processes will run in, and initial outputs will be written to, a so-called -work-dir. After each process finishes, its key output files will then be copied to the final output dir. (There are also several pipeline options to customize what will and will not be copied.)\nThe distinction between such a work-dir and a final output dir can be very useful on HPC systems like OSC: you can use a scratch dir (at OSC: /fs/scratch/) with lots of storage space and fast I/O as the work-dir, and a backed-up project dir (at OSC: /fs/ess/) as the outdir, which will then not become unnecessarily large.\n-resume:\nBesides resuming wherever the pipeline left off after an incomplete run (for example: it ran out of time or ran into an error), the -resume option also checks for any changes in input files or pipeline settings.\nFor example, if you have run the pipeline to completion previously, but rerun it after adding or replace one sample, -resume would make the pipeline only rerun the “single-sample steps” of the pipeline (which is most of them) for that sample as well as all steps that use all samples. Similarly, if you change an option that affects one of the first processes in the pipeline, the entire pipeline may be rerun, whereas if you change an option that only affects the last process, then only that last process would be rerun.\nThis option won’t make any difference when we run the pipeline for the first time, since there is nothing to resume. Nextflow will even give a warning along these lines, but this is not a problem.\n\n\n\nWith all the above-mentioned options, our final nextflow run command will be:\n# [Partial shell script code, don't copy or run]\nnextflow run software/nfc-rnaseq/3_14_0 \\\n    --input \"$samplesheet\" \\\n    --fasta \"$fasta\" \\\n    --gtf \"$gtf\" \\\n    --outdir \"$outdir\" \\\n    --remove_ribo_rna \\\n    -work-dir \"$outdir\"/raw \\\n    -profile singularity \\\n    -ansi-log false \\\n    -resume\nThe command uses several variables (e.g. \"$samplesheet\") — these will enter the script via command-line arguments.\n\n\n\n4.2 Creating an OSC configuration file\nTo speed things up and make use of the computing power at OSC, we want the pipeline to submit Slurm batch jobs for us.\nWe have to tell it to do this, and how, using a configuration (config) file. There are multiple ways of storing this file and telling Nextflow about it — the one we’ll use is to simply create a file nextflow.config in the dir from which we submit the nextflow run command: Nextflow will automatically detect and parse such a file.\nWe will keep this file as simple as possible, only providing the “executor” (in our case: the Slurm program) and the OSC project to use:\necho \"\nprocess.executor = 'slurm'\nprocess.clusterOptions='--account=PAS2658'\n\" &gt; nextflow.config\n\n\n\n4.3 Adding #SBATCH options\nWe will use #SBATCH header lines to define some parameters for our batch job for Slurm. Note that these are only for the “main” Nextflow job, not for the jobs that Nextflow itself will submit!\n#SBATCH --account=PAS2658\n#SBATCH --time=3:00:00\n#SBATCH --mail-type=END,FAIL\n#SBATCH --output=slurm-nfc_rnaseq-%j.out\n\n--account=PAS2658: As always, we have to specify the OSC project.\n--time=3:00:00: Ask for 3 hours (note that for a run of a full data set, you may want to use 6-24 hours).\n--mail-type=END,FAIL: Have Slurm send us an email when the job ends normally or with an error.\n--output=slurm-nfc_rnaseq-%j.out: Use a descriptive Slurm log file name (%j is the Slurm job number).\n\nWe only a need a single core and up to a couple GB of RAM, so the associated Slurm defaults will work for us.\n\n\n\n4.4 The final script\nWe’ve covered most of the pieces of our script. Below is the full code for the script, in which I also added:\n\nA shebang header line to indicate that this is a Bash shell script: #!/bin/bash.\nA line to use “strict Bash settings”, set -euo pipefail5.\nSome echo reporting of arguments/variables, printing the date, etc.\n\n Open your scripts/nfc-rnaseq.sh script and paste the following into it:\n#!/bin/bash\n#SBATCH --account=PAS2658\n#SBATCH --time=3:00:00\n#SBATCH --mail-type=END,FAIL\n#SBATCH --output=slurm-nfc_rnaseq-%j.out\n\n# Settings and constants\nWORKFLOW_DIR=software/nfc-rnaseq/3_14_0\n\n# Load the Nextflow Conda environment\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/nextflow\nexport NXF_SINGULARITY_CACHEDIR=/fs/ess/PAS0471/containers\n\n# Strict Bash settings\nset -euo pipefail\n\n# Process command-line arguments\nsamplesheet=$1\nfasta=$2\ngtf=$3\noutdir=$4\n\n# Report\necho \"Starting script nfc-rnaseq.sh\"\ndate\necho \"Samplesheet:          $samplesheet\"\necho \"Reference FASTA:      $fasta\"\necho \"Reference GTF:        $gtf\"\necho \"Output dir:           $outdir\"\necho\n\n# Create the output dir\nmkdir -p \"$outdir\"\n\n# Create the config file\necho \"\nprocess.executor = 'slurm'\nprocess.clusterOptions='--account=PAS2658'\n\" &gt; nextflow.config\n\n# Run the workflow\nnextflow run \"$WORKFLOW_DIR\" \\\n    --input \"$samplesheet\" \\\n    --fasta \"$fasta\" \\\n    --gtf \"$gtf\" \\\n    --outdir \"$outdir\" \\\n    --remove_ribo_rna \\\n    -work-dir \"$outdir\"/raw \\\n    -profile singularity \\\n    -ansi-log false \\\n    -resume\n\n# Report\necho \"Done with script nfc-rnaseq.sh\"\ndate\n\n\n Exercise: Take a look at the script\nGo through your complete scripts/nfc-rnaseq.sh script and see if you understand everything that’s going on in there. Ask if you’re confused about anything!"
  },
  {
    "objectID": "lab1.html#running-the-pipeline",
    "href": "lab1.html#running-the-pipeline",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "5 Running the pipeline",
    "text": "5 Running the pipeline\nWe will now switch back to the run/run.sh script to add the code to submit our script. But we’ll have to create a sample sheet first.\n\n5.1 Preparing the sample sheet\nThis pipeline requires a “sample sheet” as one of its inputs. In the sample sheet, you provide the paths to your FASTQ files and the so-called “strandedness” of your RNA-Seq library.\n\n\n\n\n\n\nRNA-Seq library strandedness\n\n\n\nDuring RNA-Seq library prep, information about the directionality of the original RNA transcripts can be retained (resulting in a “stranded” library) or lost (resulting in an “unstranded” library: specify unstranded in the sample sheet).\nIn turn, stranded libraries can prepared either in reverse-stranded (reverse, by far the most common) or forward-stranded (forward) fashion. For more information about library strandedness, see this page.\nThe pipeline also allows for a fourth option: auto, in which case the strandedness is automatically determined at the start of the pipeline by pseudo-mapping a small proportion of the data with Salmon.\n\n\nThe sample sheet should be a plain-text comma-separated values (CSV) file. Here is the example file from the pipeline’s documentation:\nsample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz,auto\nSo, we need a header row with column names, then one row per sample, and the following columns:\n\nSample ID (we will simply use the part of the file names shared by R1 and R2).\nR1 FASTQ file path (including the dir unless they are in your working dir).\nR2 FASTQ file path (idem).\nStrandedness: unstranded, reverse, forward, or auto — this data is forward-stranded, so we’ll use forward.\n\nYou can create this file in several ways — we will do it here with a helper script that comes with the pipeline6:\n\nFirst, we define an output dir (this will also be the output dir for the pipeline), and the sample sheet file name:\n# [Paste this into run/run.sh and then run it in the terminal]\n\n# Define the output dir and sample sheet file name\noutdir=results/nfc-rnaseq\nsamplesheet=\"$outdir\"/nfc_samplesheet.csv\nmkdir -p \"$outdir\"\nNext, we run that helper script, specifying the strandedness of our data, the suffices of the R1 and R2 FASTQ files, and as arguments at the end, the input FASTQ dir (data/fastq) and the output file ($samplesheet):\n# [Paste this into run/run.sh and then run it in the terminal]\n\n# Create the sample sheet for the nf-core pipeline\npython3 software/nfc-rnaseq/3_14_0/bin/fastq_dir_to_samplesheet.py \\\n    --strandedness forward \\\n    --read1_extension \"_R1.fastq.gz\" \\\n    --read2_extension \"_R2.fastq.gz\" \\\n    data/fastq \\\n    \"$samplesheet\"\nFinally, let’s check the contents of our newly created sample sheet file:\n# [Run this directly in the terminal]\ncat \"$samplesheet\"\nsample,fastq_1,fastq_2,strandedness\nERR10802863,data/fastq/ERR10802863_R1.fastq.gz,data/fastq/ERR10802863_R2.fastq.gz,forward\nERR10802864,data/fastq/ERR10802864_R1.fastq.gz,data/fastq/ERR10802864_R2.fastq.gz,forward\nERR10802865,data/fastq/ERR10802865_R1.fastq.gz,data/fastq/ERR10802865_R2.fastq.gz,forward\nERR10802866,data/fastq/ERR10802866_R1.fastq.gz,data/fastq/ERR10802866_R2.fastq.gz,forward\nERR10802867,data/fastq/ERR10802867_R1.fastq.gz,data/fastq/ERR10802867_R2.fastq.gz,forward\nERR10802868,data/fastq/ERR10802868_R1.fastq.gz,data/fastq/ERR10802868_R2.fastq.gz,forward\nERR10802869,data/fastq/ERR10802869_R1.fastq.gz,data/fastq/ERR10802869_R2.fastq.gz,forward\nERR10802870,data/fastq/ERR10802870_R1.fastq.gz,data/fastq/ERR10802870_R2.fastq.gz,forward\nERR10802871,data/fastq/ERR10802871_R1.fastq.gz,data/fastq/ERR10802871_R2.fastq.gz,forward\nERR10802874,data/fastq/ERR10802874_R1.fastq.gz,data/fastq/ERR10802874_R2.fastq.gz,forward\nERR10802875,data/fastq/ERR10802875_R1.fastq.gz,data/fastq/ERR10802875_R2.fastq.gz,forward\nERR10802876,data/fastq/ERR10802876_R1.fastq.gz,data/fastq/ERR10802876_R2.fastq.gz,forward\nERR10802877,data/fastq/ERR10802877_R1.fastq.gz,data/fastq/ERR10802877_R2.fastq.gz,forward\nERR10802878,data/fastq/ERR10802878_R1.fastq.gz,data/fastq/ERR10802878_R2.fastq.gz,forward\nERR10802879,data/fastq/ERR10802879_R1.fastq.gz,data/fastq/ERR10802879_R2.fastq.gz,forward\nERR10802880,data/fastq/ERR10802880_R1.fastq.gz,data/fastq/ERR10802880_R2.fastq.gz,forward\nERR10802881,data/fastq/ERR10802881_R1.fastq.gz,data/fastq/ERR10802881_R2.fastq.gz,forward\nERR10802882,data/fastq/ERR10802882_R1.fastq.gz,data/fastq/ERR10802882_R2.fastq.gz,forward\nERR10802883,data/fastq/ERR10802883_R1.fastq.gz,data/fastq/ERR10802883_R2.fastq.gz,forward\nERR10802884,data/fastq/ERR10802884_R1.fastq.gz,data/fastq/ERR10802884_R2.fastq.gz,forward\nERR10802885,data/fastq/ERR10802885_R1.fastq.gz,data/fastq/ERR10802885_R2.fastq.gz,forward\nERR10802886,data/fastq/ERR10802886_R1.fastq.gz,data/fastq/ERR10802886_R2.fastq.gz,forward\n\n\n\n\n\n\n\nCreating the sample sheet with shell commands instead (Click to expand)\n\n\n\n\n\n# A) Define the file name and create the header line\necho \"sample,fastq_1,fastq_2,strandedness\" &gt; \"$samplesheet\"\n  \n# B) Add a row for each sample based on the file names\nls data/fastq/* | paste -d, - - |\n    sed -E -e 's/$/,forward/' -e 's@.*/(.*)_R1@\\1,&@' &gt;&gt; \"$samplesheet\"\nHere is an explanation of the last command:\n\nThe ls command will spit out a list of all FASTQ files that includes the dir name.\npaste - - will paste that FASTQ files side-by-side in two columns — because there are 2 FASTQ files per sample, and they are automatically correctly ordered due to their file names, this will create one row per sample with the R1 and R2 FASTQ files next to each other.\nThe -d, option to paste will use a comma instead of a Tab to delimit columns.\nWe use sed with extended regular expressions (-E) and two separate search-and-replace expressions (we need -e in front of each when there is more than one).\nThe first sed expression 's/$/,forward/' will simply add ,forward at the end ($) of each line to indicate the strandedness.\nThe second sed expression, 's@.*/(.*)_R1@\\1,&@':\n\nHere we are adding the sample ID column by copying that part from the R1 FASTQ file name.\nThis uses s@&lt;search&gt;@replace@ with @ instead of /, because there is a / in our search pattern.\nIn the search pattern (.*/(.*)_R1), we capture the sample ID with (.*).\nIn the replace section (\\1,&), we recall the captured sample ID with \\1, then insert a comma, and then insert the full search pattern match (i.e., the path to the R1 file) with &.\n\nWe append (&gt;&gt;) to the file because we need to keep the header line that we had already put in it.\n\n\n\n\n\n\n\n5.2 Submitting our shell script\nAs a last preparatory step, we will save the paths of the reference genome files in variables:\n# [Paste this into run/run.sh and then run it in the terminal]\n# Define the reference genome files\nfasta=data/ref/GCF_016801865.2.fna\ngtf=data/ref/GCF_016801865.2.gtf\nBefore we submit the script, let’s check that all the variables have been assigned by prefacing the command with echo:\n# [ Run this directly in the terminal]\necho sbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\"\nsbatch scripts/nfc-rnaseq.sh results/nfc-rnaseq/nfc_samplesheet.csv data/ref/GCF_016801865.2.fna data/ref/GCF_016801865.2.gtf results/nfc-rnaseq\nNow we are ready to submit the script as a batch job:\n# [Paste this into run/run.sh and then run it in the terminal]\n# Submit the script to run the pipeline as a batch job\nsbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\"\nSubmitted batch job 27767854\n\n\n\n5.3 Checking the pipeline’s progress\nWe can check whether our job has started running, and whether Nextflow has already spawned jobs, with squeue:\n# [Run this directly in the terminal]\nsqueue -u $USER -l\nMon Mar 25 12:13:38 2024\n      JOBID PARTITION     NAME     USER    STATE   TIME TIME_LIMI  NODES NODELIST(REASON)\n  27767854 serial-40 nfc-rnas   jelmer  RUNNING    1:33   3:00:00      1 p0219\nIn the example output above, the only running job is the one we directly submitted, i.e. the main Nextflow process. Because we didn’t give the job a name, the NAME column is the script’s name, nfc-rnaseq.sh (truncated to nfc-rnas).\n\n\n\n\n\n\nSee examples of squeue output that includes Nextflow-submitted jobs (Click to expand)\n\n\n\n\n\nThe top job, with partial name nf-NFCOR, is a job that’s been submitted by Nextflow:\nsqueue -u $USER -l\nMon Mar 25 13:14:53 2024\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          27767861 serial-40 nf-NFCOR   jelmer  RUNNING       5:41  16:00:00      1 p0053\n          27767854 serial-40 nfc_rnas   jelmer  RUNNING    1:03:48   3:00:00      1 p0219\nUnfortunately, the columns in the output above are quite narrow, so it’s not possible to see which step of the pipeline is being run by that job. The following (awful-looking!) code can be used to make that column much wider, so we can see the job’s full name which makes clear which step is being run (rRNA removal with SortMeRNA):\nsqueue -u $USER --format=\"%.9i %.9P %.60j %.8T %.10M %.10l %.4C %R %.16V\"\nMon Mar 25 13:15:05 2024\n    JOBID PARTITION                                                          NAME    STATE       TIME TIME_LIMIT CPUS NODELIST(REASON)      SUBMIT_TIME\n 27767861 serial-40   nf-NFCORE_RNASEQ_RNASEQ_SORTMERNA_(SRR27866691_SRR27866691)  RUNNING       5:55   16:00:00   12 p0053 2024-03-23T09:37\n 27767854 serial-40                                                    nfc_rnaseq  RUNNING    1:04:02    3:00:00    1 p0219 2024-03-23T09:36\nYou might also catch the pipeline while there are many more jobs running, e.g.:\nMon Mar 25 13:59:50 2024\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          27823107 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0091\n          27823112 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0119\n          27823115 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0055\n          27823120 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0055\n          27823070 serial-40 nf-NFCOR   jelmer  RUNNING       0:43  16:00:00      1 p0078\n          27823004 serial-40 nfc-rnas   jelmer  RUNNING       2:13   3:00:00      1 p0146\n          27823083 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0078\n          27823084 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0096\n          27823085 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0096\n          27823086 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0115\n          27823087 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0115\n          27823088 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0123\n          27823089 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0123\n          27823090 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0057\n          27823091 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0057\n          27823092 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0058\n          27823093 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0058\n          27823095 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0118\n          27823099 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0118\n          27823103 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0119\n          27823121 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0625\n          27823122 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0744\n          27823123 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0780\n          27823124 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0780\n\n\n\nWe can keep an eye on the pipeline’s progress, and see if there are any errors, by checking the Slurm log file — the top of the file should look like this:\n# You will have a different job ID - replace as appropriate or use Tab completion\nless slurm-nfc_rnaseq-27767861.out\nStarting script nfc-rnaseq.sh\nMon Mar 25 13:01:30 EDT 2024\nSamplesheet:          results/nfc-rnaseq/nfc_samplesheet.csv\nReference FASTA:      data/ref/GCF_016801865.2.fna\nReference GTF:        data/ref/GCF_016801865.2.gtf\nOutput dir:           results/nfc-rnaseq\n\nN E X T F L O W  ~  version 23.10.1\nWARN: It appears you have never run this project before -- Option `-resume` is ignored\nLaunching `software/nfc-rnaseq/3_14_0/main.nf` [curious_linnaeus] DSL2 - revision: 746820de9b\nWARN: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  Multiple config files detected!\n  Please provide pipeline parameters via the CLI or Nextflow '-params-file' option.\n  Custom config files including those provided by the '-c' Nextflow option can be\n  used to provide any configuration except for parameters.\n\n  Docs: https://nf-co.re/usage/configuration#custom-configuration-files\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/rnaseq v3.14.0\n------------------------------------------------------\nCore Nextflow options\n  runName                   : curious_linnaeus\n  containerEngine           : singularity\n[...output truncated...]\nThe warnings about -resume and config files shown above can be ignored. Some of this output actually has nice colors:\n\n\n\n\n\nIn the Slurm log file, the job progress is show in the following way — we only see which jobs are being submitted, not when they finish7:\n[e5/da8328] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF_FILTER (GCF_016801865.2.fna)\n[b5/9427a1] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:CUSTOM_GETCHROMSIZES (GCF_016801865.2.fna)\n[05/e0e09f] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802863)\n[25/a6c2f5] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802863)\n[24/cef9a0] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802864)\n[b1/9cfa7e] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802864)\n[c4/3107c1] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802865)\n[7e/92ec89] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802866)\n[01/f7ccfb] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802866)\n[42/4b4da2] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802868)\n[8c/fe6ca5] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802867)\n[e6/a12ec8] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802867)\n[2e/f9059d] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802865)\n[de/2735d1] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802868)\n\n\nYou should also see the following warning among the job submissions (Click to expand)\n\nThis warning can be ignored, the “Biotype QC” is not important and this information is indeed simply missing from our GTF file, there is nothing we can do about that.\nWARN: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  Biotype attribute 'gene_biotype' not found in the last column of the GTF file!\n\n  Biotype QC will be skipped to circumvent the issue below:\n  https://github.com/nf-core/rnaseq/issues/460\n\n  Amend '--featurecounts_group_type' to change this behaviour.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBut any errors would be reported in this file, and we can also see when the pipeline has finished:\n[28/79e801] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRAPH_BEDCLIP_BEDGRAPHTOBIGWIG_FORWARD:UCSC_BEDGRAPHTOBIGWIG (ERR10802864)\n[e0/ba48c9] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRAPH_BEDCLIP_BEDGRAPHTOBIGWIG_REVERSE:UCSC_BEDGRAPHTOBIGWIG (ERR10802864)\n[62/4f8c0d] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:MULTIQC (1)\n-[nf-core/rnaseq] Pipeline completed successfully -\nDone with script nfc-rnaseq.sh\nMon Mar 25 14:09:52 EDT 2024"
  },
  {
    "objectID": "lab1.html#checking-the-pipelines-outputs",
    "href": "lab1.html#checking-the-pipelines-outputs",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "6 Checking the pipeline’s outputs",
    "text": "6 Checking the pipeline’s outputs\nIf your pipeline run finished in time (it may finish in as little as 15-30 minutes, but this can vary substantially8), you can take a look at the files and dirs in the output dir we specified:\nls -lh results/nfc-rnaseq\ntotal 83K\ndrwxr-xr-x   2 jelmer PAS0471  16K Mar 25 13:02 fastqc\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 12:58 logs\ndrwxr-xr-x   3 jelmer PAS0471 4.0K Mar 25 13:14 multiqc\n-rw-r--r--   1 jelmer PAS0471 2.0K Mar 25 19:55 nfc_samplesheet.csv\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 13:14 pipeline_info\ndrwxr-xr-x 248 jelmer PAS0471  16K Mar 25 13:10 raw\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 13:06 sortmerna\ndrwxr-xr-x  33 jelmer PAS0471  16K Mar 25 13:12 star_salmon\ndrwxr-xr-x   3 jelmer PAS0471 4.0K Mar 25 13:02 trimgalore\nThe two outputs we are most interested in are:\n\nThe MultiQC report (&lt;outdir&gt;/multiqc/star_salmon/multiqc_report.html): this has lots of QC summaries of the data, both the raw data and the alignments, and even a gene expression PCA plot.\nThe gene count table (&lt;outdir&gt;/star_salmon/salmon.merged.gene_counts_length_scaled.tsv): if you do the Gene count table analysis lab, you will use an equivalent file (but then run on the full data set) as the main input.\n\n\n\n6.1 The MultiQC report\nYou can find a copy of the MultiQC report on this website, here. Go ahead and open that in a separate browser tab. There’s a lot of information in the report! Here are some items to especially pay attention to, with figures from our own data set:\n\nThe General Statistics table (the first section) is very useful, with the following notes:\n\nMost of the table’s content is also in later graphs, but the table allows for comparisons across metrics.\nThe %rRNA (% of reads identified as rRNA and removed by SortMeRNA) can only be found in this table.\nIt’s best to hide the columns with statistics from Samtools, which can be confusing if not downright misleading: click on “Configure Columns” and uncheck all the boxes for stats with Samtools in their name.\nSome stats are for R1 and R2 files only, and some are for each sample as a whole. Unfortunately, this means you get 3 rows per sample in the table.\n\n\n\n\n\n\n\n\n\nThe Qualimap &gt; Genomic origin of reads plot shows, for each sample, the proportion of reads mapping to exonic vs. intronic vs. intergenic regions. This is an important QC plot: the vast majority of your reads should be exonic9.\n\n\n\n\nThis is a good result, with 80-90% of mapped reads in exonic regions.\n\n\n\n\nThe STAR &gt; Alignment Scores plot shows, for each sample, the percentage of reads that was mapped. Note that “Mapped to multiple loci” reads are also included in the final counts, and that “Unmapped: too short” merely means unmapped, really, and not that the reads were too short.\n\n\n\n\nThis is a pretty good results, with 80-90% of reads mapped.\n\n\n\n\nFastQC checks your FASTQ files, i.e. your data prior to alignment. There are FastQC plots both before and after trimming with TrimGalore/Cutadapt. The most important FastQC modules are:\n\nSequence Quality Histograms — You’d like the mean qualities to stay in the “green area”.\nPer Sequence GC Content — Secondary peaks may indicate contamination.\nAdapter Content — Any adapter content should be gone in the post-trimming plot.\n\n\n\n\n Exercise: Interpreting FastQC results in the MultiQC report\nTake a look at the three FastQC modules discussed above, both before and after trimming.\n\nHas the base quality improved after trimming, and does this look good?\n\n\n\nClick to see the answer\n\n\nPre-trimming graph: The qualities are good overall, but there is more variation that what is usual, and note the poorer qualities in the first 7 or so bases. There is no substantial decline towards the end of the read as one often sees with Illumina data, but this is expected given that the reads are only 75 bp.\n\n\n\n\nPre-trimming (Mean base quality scores: one line is one sample.)\n\n\n\nPost-trimming graph: The qualities have clearly improved. The first 7 or so bases remain of clearly poorer quality, on average.\n\n\n\n\nPost-trimming\n\n\n\n\nDo you have any idea what’s going with the pre-trimming GC content distribution? What about after trimming — does this look good or is there reason to worry?\n\n\n\nClick to see the answer\n\n\nThe pre-trimming GC content is very odd but this is mostly due to a high number of reads with zero and near-zero percent GC content. These are likely reads with only Ns. There are also some reads with near-hundred percent GC content. These are likely artifactual G-only reads that NextSeq/NovaSeq machines can produce.\n\n\n\n\nPre-trimming. One line is one file.\n\n\n\nAfter trimming, things look a lot better but there may be contamination here, given the weird “shoulder” at 30-40% GC.\n\n\n\n\nPost-trimming\n\n\n\n\nDo you know what the “adapters” that FastQC found pre-trimming are? Were these sequences removed by the trimming?\n\n\n\nClick to see the answer\n\n\nPre-trimming, there seem to be some samples with very high adapter content throughout the read. This doesn’t make sense for true adapters, because these are usually only found towards the end of the read, when the read length is longer than the DNA fragment length. If you hover over the lines, you’ll see it says “polyg”. These are artifactual G-only reads that NextSeq/NovaSeq can produce, especially in the reverse reads — and you can see that all of the lines are for reverse-read files indeed.\n\n\n\n\nPre-trimming\n\n\n\nPost-trimming, no adapter content was found.\n\n\n\n\nPost-trimming\n\n\n\n\n\n\n\n\n\n\n\nSome additional graphs of interest in the MultiQC report (Click to expand)\n\n\n\n\n\n\nThe Qualimap &gt; Gene Coverage Profile plot. This shows average read-depth across the position of genes/transcripts (for all genes together), which helps to assess the amount of RNA degradation. For poly-A selected libraries, RNA molecules “begin” at the 3’ end (right-hand side of the graph), so the more degradation there is, the more you expect there to be a higher read-depth towards the 3’ end compared to the 5’ end. (Though note that sharp decreases at the very end on each side are expected.)\n\n\n\n\nThere depth at ~20% (near the 5’ end) is clearly lower than at ~80% (near the 3’ end),indicating some RNA degradation.\n\n\n\nThe RSeqQC &gt; Infer experiment (library strandedness) plot. If your library is:\n\nUnstranded, there should be similar percentages of Sense and Antisense reads.\nForward-stranded, the vast majority of reads should be Sense.\nReverse-stranded, the vast majority of reads should be Antisense.\n\n\n\n\n\nThis libary is clearly forward-stranded, as we indicated in our sample sheet.\n\n\n\nThe STAR_SALMON DESeq2 PCA plot is from a Principal Component Analysis (PCA) run on the final gene count table, thus showing overall patterns of gene expression similarity among samples.\n\n\n\n\nThe sampels clear form two distinct groups along PC1.\n\n\n\n\n\n\n\n\n\n\n\nDid your pipeline run finish? Here’s how to check out your own MultiQC report (Click to expand)\n\n\n\n\n\nTo download the MultiQC HTML file at results/nfc-rnaseq/multiqc/star_salmon/multiqc_report.html, find this file in the VS Code explorer (file browser) on the left, right-click on it, and select Download....\nYou can download it to any location on your computer. Then find the file on your computer and click on it to open it — it should be opened in your browser.\n\n\n\n\n\n\n6.2 The gene count table\nThe gene count table has one row for each gene and one column for each sample, with the first two columns being the gene_id and gene_name10. Each cell’s value contains the read count estimate for a specific gene in a specific sample:\n# [Paste this into the run/run.sh script and run it in the terminal]\n\n# Take a look at the count table:\n# ('column -t' lines up columns, and less's '-S' option turns off line wrapping)\ncounts=results/nfc-rnaseq/star_salmon/salmon.merged.gene_counts_length_scaled.tsv\ncolumn -t \"$counts\" | less -S\ngene_id             gene_name           ERR10802863        ERR10802864        ERR10802865        ERR10802866        ERR10802867        ERR10802868       \nATP6                ATP6                163.611027228009   178.19903533081    82.1025390726658   307.649552934133   225.78249209207    171.251589309856  \nATP8                ATP8                0                  1.01047333891691   0                  0                  0                  0                 \nCOX1                COX1                1429.24769032452   2202.82009602881   764.584344577622   2273.6965332904    2784.47391614249   2000.51277019854  \nCOX2                COX2                116.537361366535   175.137972566817   54.0166352459629   256.592955351283   193.291937038438   164.125833130119  \nCOX3                COX3                872.88670991359    1178.29247734231   683.167933227141   1200.01735304529   1300.3853323715    1229.11746824104  \nCYTB                CYTB                646.028108528182   968.256051104547   529.393909319439   1025.23768317788   1201.46662840336   842.533209911258  \nLOC120412322        LOC120412322        0                  0                  0                  0                  0.995135178345792  0.996805450081561 \nLOC120412324        LOC120412324        37.8326244586681   20.9489661184365   27.6702324729125   48.6417838830061   22.8313729348804   36.87899862428    \nLOC120412325        LOC120412325        3.21074365394071   2.10702898851342   4.40315394778926   5.47978997387391   4.33241716734803   4.23386924919438  \nLOC120412326        LOC120412326        0                  0                  0                  0                  0                  0                 \nLOC120412327        LOC120412327        37.8206758601034   35.9063291323018   38.517771617566    27.7802608986967   37.6979028802121   32.885944667709   \nLOC120412328        LOC120412328        35.0080600370267   20.0019192467143   23.9260736995594   30.0191332346116   21.0383665366408   28.9844776623531  \nLOC120412329        LOC120412329        121.777922287929   112.794544755113   131.434181046282   127.753086659103   114.864750589664   131.589608063253  \nLOC120412330        LOC120412330        42.8505448763697   28.9442284428204   36.6285174684674   46.7310765909945   42.7633834468768   26.9265243413636  \nLOC120412331        LOC120412331        11.013179311581    9.00559907892481   12.9836833055803   13.029954361225    7.02624958751718   16.000552787954   \nLOC120412332        LOC120412332        12.1055360835441   26.1231316926989   21.2767913384733   18.2783703626438   26.4932540325187   22.098808637857   \nLOC120412333        LOC120412333        19.1159998132169   17.0558058070299   12.0965688236319   14.1510477997588   15.2033452089903   9.02624985028677  \nLOC120412334        LOC120412334        9.01332125155807   3.00232591636489   5.99566364212933   11.0306919231504   8.03448732510427   11.0022053123759  \n# [...output truncated...]\n\n\n\n\n\n\nCount table versions\n\n\n\nThe workflow outputs several versions of the count table11, but the one with gene_counts_length_scaled is the one we want:\n\ngene_counts as opposed to transcript_counts for counts that are summed across transcripts for each gene.\nlength for estimates that have been adjusted to account for between-sample differences in mean transcript length (longer transcripts would be expected to produce more reads in sequencing).\nscaled for estimates that have been scaled back using the “library sizes”, per-sample total read counts."
  },
  {
    "objectID": "lab1.html#footnotes",
    "href": "lab1.html#footnotes",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n These kinds of settings are more commonly specified with command options, but somewhat oddly, this is the only way we can specify that here.↩︎\n But if you want to run a pipeline yourself for your own research, make sure to use a dir that you have permissions to write to.↩︎\n Preferably in GTF (.gtf) format, but the pipeline can accept GFF/GFF3 (.gff/.gff3) format files as well.↩︎\n The default logging does not work well the output goes to a text file, as it will in our case because we will submit the script with the Nextflow command as a Slurm batch job.↩︎\n These setting will make the script abort whenever an error occurs, and it will also turn referencing unassigned/non-existing variables into an error. This is a recommended best-practice line to include in all shell scripts.↩︎\n The box below shows an alternative method with Unix shell commands↩︎\n The default Nextflow logging (without -ansi-log false) does show when jobs finish, but this would result in very messy output in a Slurm log file.↩︎\n This variation is mostly the result of variation in Slurm queue-ing times. The pipeline makes quite large resource requests, so you sometimes have to wait for a while for some jobs to start.↩︎\n A lot of intronic content may indicate that you have a lot of pre-mRNA in your data; this is more common when your library prep used rRNA depletion instead of poly-A selection. A lot of intergenic content may indicate DNA contamination. Poor genome annotation quality may also contribute to a low percentage of exonic reads. The RSeQC &gt; Read Distribution plot will show this with even more categories, e.g. separately showing UTRs.↩︎\n Which happen to be the same here, but these are usually different.↩︎\n And each version in two formats: .rds (a binary R object file type) and .tsv.↩︎"
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "Self-study lab: Gene count table analysis",
    "section": "",
    "text": "After running steps like read pre-processing, alignment, and quantification using the nf-core rnaseq workflow, or another method, you will have a gene count table. Today, with that gene count table, you will:"
  },
  {
    "objectID": "lab2.html#getting-set-up",
    "href": "lab2.html#getting-set-up",
    "title": "Self-study lab: Gene count table analysis",
    "section": "1 Getting set up",
    "text": "1 Getting set up\n\n1.1 Start an RStudio session at OSC\n\nLog in to OSC at https://ondemand.osc.edu\nClick on Interactive Apps (top bar) and then RStudio Server (all the way at the bottom)\nFill out the form as follows:\n\nCluster: Pitzer\nR version: 4.3.0\nProject: PAS2658\nNumber of hours: 3\nNode type: any\nNumber of cores: 2\n\nClick the big blue Launch button at the bottom.\nNow, you should be sent to a new page with a box at the top for your RStudio Server “job”, which should initially be “Queued” (waiting to start).\n\n\n\nClick to see a screenshot\n\n\n\n\nYour job should start running very soon, with the top bar of the box turning green and saying “Running”.\nClick Connect to RStudio Server at the bottom of the box, and an RStudio Server instance will open in a new browser tab. You’re ready to go!\n\n\n\n\n\n\n\n\nOptional: change two RStudio settings (Click to expand)\n\n\n\n\n\nFirst, prevent R from saving your “Workspace”:\n\nClick Tools (top bar, below your browser’s address bar) &gt; Global Options\nIn the pop-up window (stay on the General tab), change the settings under the “Workspace” heading to:\n\n\n\n\n\n\nWhy are we doing this? In short, the default behavior of saving and restoring your “Workspace”, which are all the items (objects) that you create during an R session, is bad practice. Instead, you should recreate your environment from a script and/or saved files with individual pieces of data, as we’ll do today.\n\nSecond, “update” your pipe symbol from %&gt;% 1 to |&gt; 2:\n\nAgain click Tools &gt; Global Options (you may still be there)\nNow go to Code tab in the side panel on the left, and check the box for Use native pipe operator, |&gt; (requires R 4.1+)\nClick OK at the bottom of the pop-up window\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 Create a new RStudio Project\nUsing an “RStudio Project” will most of all help to make sure your working directory in R is correct. To create a new RStudio Project inside your personal dir in /fs/scratch/PAS2658/&lt;your-name&gt;/Lab9:\n\nClick File (top bar, below your browser’s address bar) &gt; New Project\nIn the popup window, click Existing Directory.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\n\nClick Browse... to select your personal dir.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nIn the next window, you should be in your Home directory (abbreviated as ~), from which you can’t click your way to /fs/scratch! Instead, you’ll first have to click on the (very small!) ... highlighted in the screenshot below:\n\n\n\n\n\n\n\nType at least part of the path to your dir in /fs/scratch/PAS2658, e.g. as shown below, and click OK:\n\n\n\n\n(This doesn’t show the correct OSC project but you get the idea.)\n\n\n\nNow you should be able to browse/click the rest of the way to your Lab9 dir.\nClick Choose to pick your selected directory.\nClick Create Project.\n\n\n\n\n1.3 Create an R script\nWe’re going to write all our code in an R script instead of typing it in the console. This helps us to keep track of what we’ve been doing, and enables us to re-run our code after modifying input data or one of the lines of code.\nCreate and open a new R script by clicking File (top menu bar) &gt; New File &gt; R Script. Save this new script right away by clicking File &gt; Save As, and save it with a name like scripts/DE.R (inside the Lab9 dir which should be automatically selected).\n\n\n\n\n\n\nMake sure to type all the R code below inside your script, and then send it to the console from there.\n\n\n\nYou can send code to the console by pressing Ctrl + Enter on Windows, or Cmd + Return on a Mac.\n\n\n\n\n\n1.4 Load the necessary packages\nIn R, we need to install and then use “packages” (basically, add-ons) to perform specialized tasks like differential expression analysis3. Installing packages is quite straightforward in principle, but in RStudio Server at OSC, there can be some hiccups.\nI have therefore created a “library” (a directory with a collection of packages) for you — you can load the packages from that library, without needing to install them yourself. Copy the code below into your R script and then send it to the R console:\n\n# First, we define the dir that has the custom library:\ndyn.load(\"/fs/ess/PAS0471/jelmer/software/GLPK/lib/libglpk.so.40\", local = FALSE)\ncustom_library &lt;- \"/fs/ess/PAS0471/jelmer/R/rnaseq\"\n.libPaths(custom_library)\n\n# Then, we load all needed R packages from that library:\nlibrary(tidyverse)          # Misc. data manipulation and plotting\nlibrary(pheatmap)           # Heatmap plot\nlibrary(EnhancedVolcano)    # Volcano plot\nlibrary(DESeq2)             # Differential expression analysis\n\n\n\nThis will produce output in the R console (a lot when loading DESeq2), and some of it in orange, but all should be good unless you see explicit errors at the bottom (Click to see expected output)\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nlibrary(pheatmap)\n\n\nlibrary(EnhancedVolcano)\n\nLoading required package: ggrepel\n\n\n\nlibrary(DESeq2)\n\nLoading required package: S4Vectors\n\n\nLoading required package: stats4\n\n\nLoading required package: BiocGenerics\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    intersect, setdiff, union\n\n\nThe following objects are masked from 'package:dplyr':\n\n    combine, intersect, setdiff, union\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,\n    table, tapply, union, unique, unsplit, which.max, which.min\n\n\n\nAttaching package: 'S4Vectors'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    second, second&lt;-\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, rename\n\n\nThe following object is masked from 'package:tidyr':\n\n    expand\n\n\nThe following object is masked from 'package:utils':\n\n    findMatches\n\n\nThe following objects are masked from 'package:base':\n\n    expand.grid, I, unname\n\n\nLoading required package: IRanges\n\n\n\nAttaching package: 'IRanges'\n\n\nThe following object is masked from 'package:lubridate':\n\n    %within%\n\n\nThe following objects are masked from 'package:dplyr':\n\n    collapse, desc, slice\n\n\nThe following object is masked from 'package:purrr':\n\n    reduce\n\n\nLoading required package: GenomicRanges\n\n\nLoading required package: GenomeInfoDb\n\n\nLoading required package: SummarizedExperiment\n\n\nLoading required package: MatrixGenerics\n\n\nLoading required package: matrixStats\n\n\n\nAttaching package: 'matrixStats'\n\n\nThe following object is masked from 'package:dplyr':\n\n    count\n\n\n\nAttaching package: 'MatrixGenerics'\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n    colWeightedMeans, colWeightedMedians, colWeightedSds,\n    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n    rowWeightedSds, rowWeightedVars\n\n\nLoading required package: Biobase\n\n\nWelcome to Bioconductor\n\n    Vignettes contain introductory material; view with\n    'browseVignettes()'. To cite Bioconductor, see\n    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n\n\n\nAttaching package: 'Biobase'\n\n\nThe following object is masked from 'package:MatrixGenerics':\n\n    rowMedians\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    anyMissing, rowMedians\n\n\n\n\n\n\n1.5 Define our input files\nFor the differential expression analysis, we have the following input files:\n\nMetadata table — Metadata for the study, linking sample IDs to treatments\nGene count table — Produced by the nf-core rnaseq workflow\n\n\n# NOTE: here I am providing the path to my gene count table,\n#       but if you ran the workflow to completion, you can use your own.\n# We'll save the paths to our input files for later use\ncount_table_file &lt;- \"/fs/scratch/PAS2658/jelmer/share/results/salmon.merged.gene_counts_length_scaled.tsv\"\nmetadata_file &lt;- \"data/meta/metadata.tsv\""
  },
  {
    "objectID": "lab2.html#create-a-deseq2-object",
    "href": "lab2.html#create-a-deseq2-object",
    "title": "Self-study lab: Gene count table analysis",
    "section": "2 Create a DESeq2 object",
    "text": "2 Create a DESeq2 object\nLike in the Culex paper whose data we are working with, we will perform a Principal Component Analysis (PCA) and a Differential Expression (DE) analysis using the popular DESeq2 package (paper, website).\nThe DESeq2 package has its own “object type” (a specific R format type) and before we can do anything else, we need to create a DESeq2 object from three components:\n\nMetadata\nOur independent variables should be in the metadata, allowing DESeq2 to compare groups of samples.\nCount table\nA matrix (table) with one row per gene, and one column per sample.\nA statistical design\nA statistical design formula (basically, which groups to compare) will tell DESEq2 how to analyze the data\n\n\n\n2.1 Metadata\nFirst, we’ll load the metadata file and take a look at the resulting data frame:\n\n# Read in the count table\nmeta_raw &lt;- read_tsv(metadata_file, show_col_types = FALSE)\n\n\n# Take a look at the first 6 rows\nhead(meta_raw)\n\n# A tibble: 6 × 3\n  sample_id   time  treatment  \n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;      \n1 ERR10802882 10dpi cathemerium\n2 ERR10802875 10dpi cathemerium\n3 ERR10802879 10dpi cathemerium\n4 ERR10802883 10dpi cathemerium\n5 ERR10802878 10dpi control    \n6 ERR10802884 10dpi control    \n\n\nWe’ll make sure the data frame is sorted by sample ID, and that the sample IDs are contained in “row names”:\n\n# Prepare the metadata so it can be loaded into DESeq2\nmeta &lt;- meta_raw |&gt;\n  # 1. Sort by the 'sample_id' column\n  arrange(sample_id) |&gt;\n  # 2. Turn the 'sample_id' column into row names:\n  column_to_rownames(\"sample_id\") |&gt;\n  # 3. Turn the 'time' and 'treatment' columns into \"factors\":\n  mutate(time = factor(time, levels = c(\"24hpi\", \"10dpi\")),\n         treatment = factor(treatment, levels = c(\"control\", \"cathemerium\", \"relictum\")))\n\n\n# Take a look at the first 6 rows\nhead(meta)\n\n             time   treatment\nERR10802863 24hpi     control\nERR10802864 24hpi cathemerium\nERR10802865 24hpi    relictum\nERR10802866 24hpi     control\nERR10802867 24hpi cathemerium\nERR10802868 24hpi    relictum\n\n\n\n\n\n\n\n\nIn the two outputs above, note the difference between having the sample IDs as a separate column versus as row names.\n\n\n\n\n\n\n\n\n\nFactors are a common R data type for categorical variables (Click to expand)\n\n\n\n\n\nWe changed the two independent variable columns (time and treatment) into factors, because DESEq2 wants this — this also allowed us to use a custom, non-alphanumeric ordering where 24hpi comes before 10dpi:\n\nhead(meta$time)\n\n[1] 24hpi 24hpi 24hpi 24hpi 24hpi 24hpi\nLevels: 24hpi 10dpi\n\n\n\n\n\n\n\n\n2.2 Gene count table\nSecond, load the gene count table into R:\n\n# Read in the count table\ncount_df &lt;- read_tsv(count_table_file, show_col_types = FALSE)\n\n\n# Take a look at the first 6 rows\nhead(count_df)\n\n# A tibble: 6 × 24\n  gene_id gene_name ERR10802863 ERR10802864 ERR10802865 ERR10802866 ERR10802867\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 ATP6    ATP6         10275.       8255.       4103.      18615.      11625.  \n2 ATP8    ATP8             3.85        2.92        2.33        7.76        7.01\n3 COX1    COX1         88041.      83394.      36975.     136054.     130863.  \n4 COX2    COX2          8749.       7925.       2901.      16802.      10026.  \n5 COX3    COX3         55772.      50312.      35074.      80510.      69850.  \n6 CYTB    CYTB         38543.      36352.      22185.      62147.      57461.  \n# ℹ 17 more variables: ERR10802868 &lt;dbl&gt;, ERR10802869 &lt;dbl&gt;, ERR10802870 &lt;dbl&gt;,\n#   ERR10802871 &lt;dbl&gt;, ERR10802874 &lt;dbl&gt;, ERR10802875 &lt;dbl&gt;, ERR10802876 &lt;dbl&gt;,\n#   ERR10802877 &lt;dbl&gt;, ERR10802878 &lt;dbl&gt;, ERR10802879 &lt;dbl&gt;, ERR10802880 &lt;dbl&gt;,\n#   ERR10802881 &lt;dbl&gt;, ERR10802882 &lt;dbl&gt;, ERR10802883 &lt;dbl&gt;, ERR10802884 &lt;dbl&gt;,\n#   ERR10802885 &lt;dbl&gt;, ERR10802886 &lt;dbl&gt;\n\n\nAgain, we have to make several modifications before we can include it in the DESeq2 object. DESeq2 expects with whole numbers (integers) and with gene IDs as row names:\n\n# Prepare the count table so it can be loaded into DESeq2\ncount_mat &lt;- count_df |&gt;\n  # 1. Turn the 'gene_id' column into row names:\n  column_to_rownames(\"gene_id\") |&gt;\n  # 2. Remove a remaining non-numeric column (which has gene names):\n  select(-gene_name) |&gt;\n  # 3. Round everything to whole numbers:\n  round() |&gt;\n  # 4. Convert it to a formal 'matrix' format:\n  as.matrix()\n\n\n# Take a look at the first 6 rows\nhead(count_mat)\n\n     ERR10802863 ERR10802864 ERR10802865 ERR10802866 ERR10802867 ERR10802868\nATP6       10275        8255        4103       18615       11625        7967\nATP8           4           3           2           8           7           2\nCOX1       88041       83394       36975      136054      130863       62279\nCOX2        8749        7925        2901       16802       10026        6701\nCOX3       55772       50312       35074       80510       69850       42478\nCYTB       38543       36352       22185       62147       57461       28159\n     ERR10802869 ERR10802870 ERR10802871 ERR10802874 ERR10802875 ERR10802876\nATP6       12788        4408       13648       13834        1346       10032\nATP8           2           0           2           1           3           2\nCOX1      109596      106402      104394       77682       38276       78290\nCOX2       11494        6603       11151        9893        1473       13146\nCOX3       68228       71945       66900       52368       14665       37275\nCYTB       46219       52035       46090       35247       17449       38762\n     ERR10802877 ERR10802878 ERR10802879 ERR10802880 ERR10802881 ERR10802882\nATP6         987        1834        3337        5036        1983       11586\nATP8           0           0           0           3           0          27\nCOX1       17785       32099       64490       63960       50965       76113\nCOX2        1141        1907        3439        8334        2063       12752\nCOX3        8797       15948       26278       29997       17802       35419\nCYTB       11177       22262       34368       33401       25854       43912\n     ERR10802883 ERR10802884 ERR10802885 ERR10802886\nATP6       18821        2792       11749        6682\nATP8          40           0           8           1\nCOX1      108343       65829      107741       94682\nCOX2       19148        2713       17947       10656\nCOX3       51441       24915       50029       47750\nCYTB       57844       34616       50587       51198\n\n\n\nCheck that the sample IDs match\nWhen creating the DESeq2 object, DESeq2 assumes that sample IDs in both tables match and are provided in the same order. Let’s make sure this is indeed the case:\n\n# Check that sample IDs in the metadata and the count table match\nall(row.names(meta) == colnames(count_mat))\n\n[1] TRUE\n\n\n\n\n\n\n2.3 Create the DESeq2 object\nWe will create the DESeq2 object using the function DESeqDataSetFromMatrix(), which we will provide with three arguments corresponding to the components discussed above:\n\nThe metadata with argument colData.\nThe count data with argument countData.\nThe statistical design for the DE analysis with argument design. For now, we will specify ~1, which effectively means “no design” — we will change this before the actual DE analysis.\n\n\n# Create the DESeq2 object\n# (`dds` is a name commonly used for DESeq2 objects, short for \"DESeq Data Set\")\ndds &lt;- DESeqDataSetFromMatrix(\n  colData = meta,\n  countData = count_mat,\n  design = ~ 1\n  )\n\nconverting counts to integer mode\n\n\nBefore we will run the differential expression analysis, though, we will do a bit of exploratory data analysis using our dds object."
  },
  {
    "objectID": "lab2.html#exploratory-data-analysis",
    "href": "lab2.html#exploratory-data-analysis",
    "title": "Self-study lab: Gene count table analysis",
    "section": "3 Exploratory Data Analysis",
    "text": "3 Exploratory Data Analysis\n\n3.1 Our count matrix\nWhat are the number of rows (=number of genes) and columns (=number of samples) of our count matrix?\n\ndim(count_mat)\n\n[1] 18855    22\n\n\nHow many genes have total (= across all samples) counts that are non-zero?\n\nnrow(count_mat[rowSums(count_mat) &gt; 0, ])\n\n[1] 17788\n\n\n\n\n Exercise: gene counts\n\nHow many genes have total counts of at least 10?\n\n\n\nClick to see the solution\n\n\nnrow(count_mat[rowSums(count_mat) &gt;= 10, ])\n\n[1] 16682\n\n\n\n\nBonus: How many genes have mean counts of at least 10?\n\n\n\nClick to see the solution\n\n\n# Now we need to divide by the number of samples, which is the number of columns,\n# which we can get with 'ncol'\nnrow(count_mat[rowSums(count_mat) / ncol(count_mat) &gt;= 10, ])\n\n[1] 12529\n\n\n\n\n\nHow do the “library sizes”, i.e. the summed per-sample gene counts, compare across samples?\n\ncolSums(count_mat)\n\nERR10802863 ERR10802864 ERR10802865 ERR10802866 ERR10802867 ERR10802868 \n   24297245    17177436    22745445    26849403    21471477    17506262 \nERR10802869 ERR10802870 ERR10802871 ERR10802874 ERR10802875 ERR10802876 \n   24299398    25490128    26534405    22194841    18927885    28804150 \nERR10802877 ERR10802878 ERR10802879 ERR10802880 ERR10802881 ERR10802882 \n    9498249    14807513    20667093    23107463    17545375    19088206 \nERR10802883 ERR10802884 ERR10802885 ERR10802886 \n   21418234    19420046    24367372    25452228 \n\n\n\n Bonus exercise: nicer counts\nThat’s not so easy to read / interpret. Can you instead get these numbers in millions, rounded to whole numbers, and sorted from low to high?\n\n\nClick to see the solution\n\n\nsort(round(colSums(count_mat) / 1000000))\n\nERR10802877 ERR10802878 ERR10802864 ERR10802868 ERR10802881 ERR10802875 \n          9          15          17          18          18          19 \nERR10802882 ERR10802884 ERR10802867 ERR10802879 ERR10802883 ERR10802874 \n         19          19          21          21          21          22 \nERR10802865 ERR10802880 ERR10802863 ERR10802869 ERR10802885 ERR10802870 \n         23          23          24          24          24          25 \nERR10802886 ERR10802866 ERR10802871 ERR10802876 \n         25          27          27          29 \n\n\n\n\n\n\n\n3.2 Principal Component Analysis (PCA)\nWe will run a PCA to examine overall patterns of (dis)similarity among samples, helping us answer questions like:\n\nDo the samples cluster by treatment (infection status) and/or time point?\nWhich of these two variables has a greater effect on overall patterns of gene expression?\nIs there an overall interaction between these two variables?\n\nFirst, normalize the count data to account for differences in library size among samples and “stabilize” the variance among genes4:\n\ndds_vst &lt;- varianceStabilizingTransformation(dds)\n\n\n\n\n\n\n\nThe authors of the study did this as well:\n\n\n\n\nWe carried out a Variance Stabilizing Transformation (VST) of the counts to represent the samples on a PCA plot.\n\n\n\n\nNext, run and plot the PCA with a single function call, plotPCA from DESeq2:\n\n# With 'intgroup' we specify the variables (columns) to color samples by\nplotPCA(dds_vst, intgroup = c(\"time\", \"treatment\"))\n\nusing ntop=500 top features by variance\n\n\n\n\n\n\n\n Exercise: PCA\n\nBased on your PCA plot, try to answer the three questions asked at the beginning of this PCA section.\nHow does our plot compare to the PCA plot in the paper (Figure 1), in terms of the conclusions you just drew in the previous exercise.\n\n\n\nClick to see the paper’s Figure 1\n\n\n\n\n\n\nBonus: Compare the PCA plot with different numbers of included genes (Hint: figure out how to do so by looking at the help by running ?plotPCA).\nBonus: Customize the PCA plot — e.g. can you “separate” treatment and time point (different shapes for one variable, and different colors for the other), like in Fig. 1 of the paper?\n\n\n\nClick to see some hints for PCA plot customization\n\nTo expand on the point of the exercise: in the plot we made above, each combination of time point and treatment has a distinct color — it would be better to use point color only to distinguish one of the variables, and point shape to distinguish the other variable (as was also done in the paper’s Fig. 1).\nTo be able to customize the plot properly, we best build it from scratch ourselves, rather than using the plotPCA function. But then how do we get the input data in the right shape?\nA nice trick is that we can use returnData = TRUE in the plotPCA function, to get plot-ready formatted data instead of an actual plot:\n\npca_df &lt;- plotPCA(dds_vst, ntop = 500,\n                  intgroup = c(\"time\", \"treatment\"), returnData = TRUE)\n\nWith that pca_df dataframe in hand, it will be relatively straightforward to customize the plot, if you know some ggplot2.\n\n\n\nClick to see a possible solution\n\nFirst, we’ll get the data in the right format, as explained in the hint:\n\npca_df &lt;- plotPCA(dds_vst, ntop = 500,\n                  intgroup = c(\"time\", \"treatment\"), returnData = TRUE)\n\nSecond, we’ll extract and store the percentage of variance explained by different principal components, so we can later add this information to the plot:\n\npct_var &lt;- round(100 * attr(pca_df, \"percentVar\"), 1)\npct_var\n\n[1] 85.3  3.1\n\n\nNow we can make the plot:\n\nggplot(pca_df,\n       aes(x = PC1, y = PC2, color = treatment, shape = time)) +\n  geom_point(size = 5) +\n  labs(x = paste0(\"PC1 (\", pct_var[1], \"%)\"),\n       y = paste0(\"PC2 (\", pct_var[2], \"%)\")) +\n  scale_color_brewer(palette = \"Dark2\", name = \"Infection status\") +\n  scale_shape(name = \"Time points\") +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())"
  },
  {
    "objectID": "lab2.html#differential-expression-de-analysis",
    "href": "lab2.html#differential-expression-de-analysis",
    "title": "Self-study lab: Gene count table analysis",
    "section": "4 Differential Expression (DE) analysis",
    "text": "4 Differential Expression (DE) analysis\n\n4.1 Figuring out how to do the analysis\nFirst, let’s see how the DE analysis was done in the paper:\n\nThen, we used the DESeq2 package (Love et al., 2014) to perform the differential gene expression analysis comparing: (i) P. relictum-infected mosquitoes vs. controls, (ii) P. cathemerium-infected mosquitoes vs. controls, and (iii) P. relictum-infected mosquitoes vs. P. cathemerium-infected mosquitoes.\n\nThis is not terribly detailed and could be interpreted in a couple of different ways. For example, they may have compared infection statuses by ignoring time points or by controlling for time points (and there are different ways to do the latter).\nIgnoring time would mean analyzing the full dataset (all time points) while only using the infection status as an independent variable, i.e. the design ~treatment.\n\n\n Given the PCA results, do you think that ignoring the time variable is a good idea? (Click for the answer)\n\nNo: the time variable clearly has a large effect on overall patterns of gene expression, in fact more so than the treatment..\n\nControlling for time can be done in two ways:\n\nA two-factor analysis: ~ time + treatment.\nPairwise comparisons between each combination of time and treatment (we’ll see below how we can do that).\n\nIf we take a look at Table 1 with the DE results, it will become clearer how they did their analysis:\n\n\n\n\n\n\n\n How do you interpret this: did they run pairwise comparisons or a two-factor model? (Click for the answer)\n\nIt looks like they performed pairwise comparisons between each combination of time and treatment.\n\n\nThat brings us a step closer, but pairwise comparisons with &gt;1 independent variable can (also!) be done in two ways:\n\nAfter subsetting the dataset to each combination of time and treatment.\nAfter creating a single, combined independent variable that is a combination of time and treatment.\n\nThe latter method is the more common one, and is what we will do below5.\n\n\n\n4.2 Setting the statistical design\nWe will now create a new variable that is a combination of treatment and time, and call it group:\n\n# Create a combined variable called 'group':\ndds$group &lt;- factor(paste(dds$treatment, dds$time, sep = \"_\"))\n\n\n# Which unique values does 'group' have, and how many samples are in each?\ntable(dds$group)\n\n\ncathemerium_10dpi cathemerium_24hpi     control_10dpi     control_24hpi \n                4                 3                 4                 3 \n   relictum_10dpi    relictum_24hpi \n                4                 4 \n\n\nNext, we set the analysis design:\n\n# Set the statistical design (Note: the symbol before 'group' is a tilde, ~ )\ndesign(dds) &lt;- ~ group\n\nNow we’re ready to run the DE analysis!\n\n\n\n4.3 Running the DE analysis\nWhile we had to do a lot of prep to get to this stage, actually running the DE analysis is very simple:\n\n# Run the DE analysis\n# (We are assigning the output back to the same `dds` object - the DE results are added to it)\ndds &lt;- DESeq(dds)\n\nestimating size factors\n\n\nestimating dispersions\n\n\ngene-wise dispersion estimates\n\n\nmean-dispersion relationship\n\n\nfinal dispersion estimates\n\n\nfitting model and testing\n\n\nThe DESeq() function is a wrapper that performs three steps (functions) consecutively:\n\nestimateSizeFactors() — “Normalization” by library size and composition.\nestimateDispersions() — Estimate gene-wise dispersion (variance in counts).\nnbinomWaldTest(ddsObj) — Fit the negative binomial GLM and calculate test statistics\n\nA key thing to understand is that above, DESeq2 automatically performed pairwise comparisons between each of the (6) levels of the group variable. This means that for any individual gene, it tested whether the gene is differentially expressed separately for each of these pairwise comparisons."
  },
  {
    "objectID": "lab2.html#extracting-the-de-results",
    "href": "lab2.html#extracting-the-de-results",
    "title": "Self-study lab: Gene count table analysis",
    "section": "5 Extracting the DE results",
    "text": "5 Extracting the DE results\nDESeq2 stores the results as a separate table for each pairwise comparison, and now, we’ll extract one of these.\n\n5.1 The results table\nWe can extract the results for one pairwise comparison (which DESeq2 refers to as a contrast) at a time, by specifying it with the contrast argument as a vector of length 3:\n\nThe focal independent variable (here, group)\nThe first (reference) level of the independent variable (in the example below, relictum_24hpi)\nThe second level of the independent variable (in the example below, control_24hpi)\n\n\n# Extract the DE results for one pairwise comparison\nfocal_contrast &lt;- c(\"group\", \"relictum_24hpi\", \"control_24hpi\")\nres_rc24 &lt;- results(dds, contrast = focal_contrast)\n\nhead(res_rc24)\n\nlog2 fold change (MLE): group relictum_24hpi vs control_24hpi \nWald test p-value: group relictum_24hpi vs control_24hpi \nDataFrame with 6 rows and 6 columns\n       baseMean log2FoldChange     lfcSE      stat    pvalue      padj\n      &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nATP6  7658.0445      -0.416305  0.609133 -0.683438 0.4943300  0.776172\nATP8     4.9196      -1.311116  1.388811 -0.944057 0.3451406        NA\nCOX1 75166.8670      -0.590935  0.282075 -2.094958 0.0361747  0.208045\nCOX2  7807.1848      -0.610152  0.578401 -1.054893 0.2914743  0.615249\nCOX3 41037.7359      -0.400173  0.251760 -1.589498 0.1119479  0.388880\nCYTB 36916.6130      -0.501653  0.261927 -1.915242 0.0554617  0.266528\n\n\nWhat do the columns in this table contain?\n\nbaseMean: Mean expression level across all samples.\nlog2FoldChange: The “log2-fold change” of gene counts between the compared levels.\nlfcSE: The uncertainty in terms of the standard error (SE) of the log2-fold change estimate.\nstat: The value for the Wald test’s test statistic.\npvalue: The uncorrected p-value from the Wald test.\npadj: The multiple-testing corrected p-value (i.e., adjusted p-value).\n\n\n\n\n\n\n\nMultiple testing correction\n\n\n\nBecause we are testing significance for many genes, we need to correct for multiple testing. DESeq2 uses the Benjamini-Hochberg False Discovery Rate (FDR) correction. For more info, see this StatQuest video.\n\n\n\n\n\n\n\n\nLog2-fold changes (LFCs)\n\n\n\nIn RNA-seq, log2-fold changes (LFCs) are the standard way of representing the magnitude (effect size) of expression level differences between two groups of interest. With A and B being the compared sample groups, the LFC is calculated as:\nlog2(mean of A / mean of B)\nDue the log-transformation, the LFC also increase more slowly than a raw fold-change:\n\nAn LFC of 1 indicates a 2-fold difference\nAn LFC of 2 indicates a 4-fold difference\nAn LFC of 3 indicates a 8-fold difference\n\nA nice property of LFC is that decreases and increases in expression are expressed symmetrically:\n\nAn LFC of 1 means that group A has a two-fold higher expression that group B\nAn LFC of -1 means that group A has a two-fold lower expression that group B\n\n\n\n\n\n Exercise: Log-fold changes\nBased on the above, or your knowledge of log-transformations, what do you expect the following to return:\n\n# In the context of a LFC, these 2 numbers would be mean expression levels in 2 groups\nlog2(8 / 2)\nlog2(2 / 8)\n\n\n\nClick to see the solution\n\n\nA fold-change of 4 (8/2) is a LFC of 2:\n\n\nlog2(8 / 2)\n\n[1] 2\n\n\n\nA fold-change of 0.25 (2/8) is a LFC of -2:\n\n\nlog2(2 / 8)\n\n[1] -2\n\n\n\n\n\n\n\n5.2 Numbers of Differentially Expressed Genes (DEGs)\nHow many adjusted p-values were less than 0.05 (i.e., significant)?\n\n# (We need 'na.rm = TRUE' because some p-values are 'NA')\n# (If we don't remove NAs from the calculation, sum() will just return NA)\nsum(res_rc24$padj &lt; 0.05, na.rm = TRUE)\n\n[1] 801\n\n\nSo, we have 801 Differentially Expressed Genes (DEGs) for this specific pairwise comparison.\n\n\n Exercise: DEGs\nThe paper’s Table 1 (which we saw above) reports the number of DEGs for a variety of comparisons.\n\nHow does the number of DEGs we just got compare to what they found in the paper for this comparison?\nThe table also reports numbers of up- and downregulated genes separately. Can you find this out for our DEGs?\n\n\n\nClick to see the solution\n\n\nSolution using tidyverse/dplyr:\n\n\n# First we need to convert the results table into a regular data frame\nas.data.frame(res_rc24) |&gt;\n  # Then we only select the rows/genes that are significant\n  filter(padj &lt; 0.05) |&gt;\n  # If we run count() on a logical test, we get the nrs. that are FALSE v. TRUE\n  dplyr::count(log2FoldChange &gt; 0)\n\n  log2FoldChange &gt; 0   n\n1              FALSE 616\n2               TRUE 185\n\n\n\nSolution using base R:\n\n\n# Down-regulated (relictum &lt; control):\nsum(res_rc24$log2FoldChange &lt; 0 & res_rc24$padj &lt; 0.05, na.rm = TRUE)\n\n[1] 616\n\n# Up-regulated (relictum &gt; control):\nsum(res_rc24$log2FoldChange &gt; 0 & res_rc24$padj &lt; 0.05, na.rm = TRUE)\n\n[1] 185\n\n\n\n\n\n\nBonus: The table also reports the number of DEGs with an absolute LFC &gt; 1. Can you find this out for our DEGs?\n\n\n\nClick to see the solution\n\n\nSolution using tidyverse/dplyr:\n\n\n# First we need to convert the results table into a regular data frame\nas.data.frame(res_rc24) |&gt;\n  # Then we only select the rows/genes that are significant\n  filter(padj &lt; 0.05, abs(log2FoldChange) &gt; 1) |&gt;\n  # If we run count() on a logical test, we get the nrs. that are FALSE v. TRUE\n  dplyr::count(log2FoldChange &gt; 0)\n\n  log2FoldChange &gt; 0   n\n1              FALSE 159\n2               TRUE  49\n\n\n\nSolution using base R:\n\n\n# Down-regulated (relictum &lt; control):\nsum(res_rc24$log2FoldChange &lt; -1 & res_rc24$padj &lt; 0.05, na.rm = TRUE)\n\n[1] 159\n\n# Up-regulated (relictum &gt; control):\nsum(res_rc24$log2FoldChange &gt; 1 & res_rc24$padj &lt; 0.05, na.rm = TRUE)\n\n[1] 49\n\n\n\n\nBonus: Extract the results for one or more other contrasts in the table, and compare the results."
  },
  {
    "objectID": "lab2.html#visualizing-the-de-results",
    "href": "lab2.html#visualizing-the-de-results",
    "title": "Self-study lab: Gene count table analysis",
    "section": "6 Visualizing the DE results",
    "text": "6 Visualizing the DE results\nTo practice with visualization of the differential expression results, we will create a few plots for the results for the relictum_24hpi vs. control_24hpi comparison, which we extracted above.\n\n\n6.1 Volcano plot\nFor a nice overview of the results, we can create a so-called “volcano plot” using the EnhancedVolcano() function from the package of the same name (see here for a “vignette”/tutorial):\n\nEnhancedVolcano(\n  toptable = res_rc24,      # DESeq2 results to plot   \n  title = \"relictum vs. control at 24 hpi\",\n  x = \"log2FoldChange\",     # Plot the log2-fold change along the x-axis\n  y = \"padj\",               # Plot the p-value along the y-axis\n  lab = rownames(res_rc24), # Use the rownames for the gene labels (though see below)\n  labSize = 0               # Omit gene labels for now\n  )\n\n\n\n\n\n\n\n\n\n Bonus exercise: Volcano plots\nThe EnhancedVolcano() function by default adds gene IDs to highly significant genes, but above, we turned off gene name labeling by setting labSize = 0. I did this because the default p-value cut-off for point labeling is 1e-5 and in this case, that would make the plot quite busy with gene labels. We might want to try a plot with a stricter p-value cut-off that does show the gene labels.\n\nPlay around with the p-value cut-off and the labeling to create a plot you like.\nCheck the vignette, or the help page (accessed by running ?EnhancedVolcano) to see how you can do this.\n\n\n\nClick for an example\n\n\nEnhancedVolcano(\n  toptable = res_rc24,      \n  title = \"relictum vs. control at 24 hpi\",\n  x = \"log2FoldChange\",     \n  y = \"padj\",             \n  lab = rownames(res_rc24), \n  labSize = 4,               # Now we will show the gene labels\n  pCutoff = 10e-10,          # Modify the p-value cut-off\n  subtitle = NULL,           # I'll also remove the silly subtitle\n  caption = NULL,            # ... and the caption\n  )\n\n\n\n\n\n\n\n\n\n\nFigure out the identity of the above-mentioned log2-fold change outlier.\n(You can do so either by labeling it in the plot, or by filtering the res_rc24 table.)\n\n\n\nClick for the solution for how to lab it in the plot\n\n\n\nEnhancedVolcano(\n  toptable = res_rc24,      \n  title = \"relictum vs. control at 24 hpi\",\n  x = \"log2FoldChange\",     \n  y = \"padj\",             \n  lab = rownames(res_rc24), \n  labSize = 4,               \n  pCutoff = 0.05,            # Modify the p-value cut-off\n  FCcutoff = 20,             # Modify the LFC cut-off\n  )\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_vline()`).\n\n\n\n\n\n\n\n\n\n\n\n\nClick for the solution for how to find it in the results table\n\n\n\nas.data.frame(res_rc24) |&gt; filter(log2FoldChange &gt; 20)\n\n              baseMean log2FoldChange    lfcSE     stat       pvalue\nLOC120413430  7.540043       24.46898 5.397990 4.532979           NA\nLOC120431476 39.720375       23.01445 5.301369 4.341228 1.416886e-05\n                     padj\nLOC120413430           NA\nLOC120431476 0.0008584398\n\n\n(Interestingly, there’s a second gene with a LFC &gt; 20 that we hadn’t seen in the plot, because it has NA as the pvalue and padj. See the section “Extra info: NA values in the results table” in the Appendix for why p-values can be set to NA.)\n\n\n\n\n\n6.2 Plot specific genes\nWe can also create plots of expression levels for individual genes. That is especially interesting for genes with highly significant differential expression. So let’s plot the most highly significant DEG.\nFirst, let’s create a vector with most highly significant DEGs, which we’ll use again for the heatmap below.\n\ntop25_DE &lt;- row.names(res_rc24[order(res_rc24$padj)[1:25], ])\n\ntop25_DE\n\n [1] \"LOC120423768\" \"LOC120423767\" \"LOC120414587\" \"LOC128092307\" \"LOC120431154\"\n [6] \"LOC120427827\" \"LOC120415152\" \"LOC120422735\" \"LOC120431739\" \"LOC120431733\"\n[11] \"LOC120428214\" \"LOC120427588\" \"LOC120415540\" \"LOC120415522\" \"LOC120429000\"\n[16] \"LOC120414889\" \"LOC120413491\" \"LOC120414802\" \"LOC120423826\" \"LOC120429211\"\n[21] \"LOC120425480\" \"LOC120431003\" \"LOC120421894\" \"LOC120423819\" \"LOC128093166\"\n\n\nDESeq2 has a plotting function but the plot is not very good. We will still use that function but just to quickly extract the counts for our gene of interest in the right format for plotting, using returnData = TRUE:\n\nfocal_gene_counts &lt;- plotCounts(\n  dds,\n  gene = top25_DE[1],\n  intgroup = c(\"time\", \"treatment\"),\n  returnData = TRUE\n  )\n\nhead(focal_gene_counts)\n\n                 count  time   treatment\nERR10802863 1543.81532 24hpi     control\nERR10802864 2279.03704 24hpi cathemerium\nERR10802865   25.42295 24hpi    relictum\nERR10802866 1105.75009 24hpi     control\nERR10802867 1199.28425 24hpi cathemerium\nERR10802868   32.14394 24hpi    relictum\n\n\nNow, we can make the plot:\n\nggplot(focal_gene_counts,\n       # Treatment along the x-axis, gene counts along the y, color by treatment:\n       aes(x = treatment, y = count, fill = treatment)) +\n  # Plot separate \"facets\" with the different time points\n  facet_wrap(vars(time)) +\n  # Add a boxplot with a partly transparent (alpha) color:\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  # _And_ add individual points:\n  geom_point(size = 4, shape = 21,\n             position = position_jitter(w = 0.1, h = 0)) +\n  # Plot styling (e.g., we don't need a legend)\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n Exercise: Single-gene plots\n\nPlot one or a few more of the top-DE genes. Do they have similar expression patterns across treatment and time points as the first one?\n\n\n\nBonus: Plot the gene with the very high LFC value that we saw when making the volcano plot. How would you interpret this?\n\n\n\nClick for the solution\n\n\nfocal_gene_counts &lt;- plotCounts(\n  dds,\n  gene = \"LOC120431476\",\n  intgroup = c(\"time\", \"treatment\"),\n  returnData = TRUE\n  )\n\nggplot(focal_gene_counts, aes(x = treatment, y = count, fill = treatment)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  geom_point(size = 4, shape = 21, position = position_jitter(w = 0.1, h = 0)) +\n  facet_wrap(vars(time)) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWow! It looks like in every single time + treatment combinations, all but one (or in one case, two) of the samples have zero expression, but there are several extreme outliers.\nOur focal comparison at 24hpi (left panel/facet), and comparing control vs relictum: so it looks like the difference between these two groups is solely due to the one outlier in relictum. Nevertheless, even the multiple-testing corrected p-value (padj) is significant for this gene:\n\nas.data.frame(res_rc24) |&gt;\n  rownames_to_column(\"gene\") |&gt;\n  filter(gene == \"LOC120431476\")\n\n          gene baseMean log2FoldChange    lfcSE     stat       pvalue\n1 LOC120431476 39.72038       23.01445 5.301369 4.341228 1.416886e-05\n          padj\n1 0.0008584398\n\n\nSo, we have to be careful with talking our statistical results at face value, and need to visualize important genes!\n\n\n\n\n\n\n\nOutliers!\n\n\n\nYou may want to check out the solution to the previous exercise, even if you don’t get around to doing it yourself."
  },
  {
    "objectID": "lab2.html#in-closing",
    "href": "lab2.html#in-closing",
    "title": "Self-study lab: Gene count table analysis",
    "section": "7 In Closing",
    "text": "7 In Closing\nToday, you have performed several steps in the analysis of gene counts that result from a typical RNA-seq workflow. Specifically, you have:\n\nCreated a DESEq2 object from the gene count data and the experiment’s metadata\nPerformed exploratory data analysis including a PCA\nRan a Differential Expression (DE) analysis with DESeq2\nExtracted, interpreted, and visualized the DE results\n\n\nNext steps\nTypical next steps in such an analysis include:\n\nExtracting, comparing, and synthesizing DE results across all pairwise comparisons (this would for example allow us to make the upset plot in Figure 2 of the paper)\nFunctional enrichment analysis with Gene Ontology (GO) terms, as done in the paper, and/or with KEGG pathways and other functional gene grouping systems."
  },
  {
    "objectID": "lab2.html#appendix",
    "href": "lab2.html#appendix",
    "title": "Self-study lab: Gene count table analysis",
    "section": "8 Appendix",
    "text": "8 Appendix\n\n8.1 Heatmaps\nRather than plotting expression levels for many individual genes, we can create “heatmap” plots to plot dozens (possibly even hundreds) of genes at once.\nWe will create heatmaps with the pheatmap function, and let’s make a heatmap for the top-25 most highly significant DEGs for our focal contrast.\nUnlike with some of the functions we used before, we unfortunately can’t directly use our DESeq2 object, but we have to extract and subset the count matrix, and also pass the metadata to the heatmap function:\n\n# We need a normalized count matrix, like for the PCA\n# We can simply extract the matrix from the normalized dds object we created for the PCA\nnorm_mat &lt;- assay(dds_vst)\n\n# In the normalized count matrix, select only the genes of interest\n# We'll reuse the 'top25_DE' vector that we created for the individual gene plots\nnorm_mat_sel &lt;- norm_mat[match(top25_DE, rownames(norm_mat)), ]\n\n# Sort the metadata\nmeta_sort &lt;- meta |&gt;\n  arrange(treatment, time) |&gt;\n  select(treatment, time)\n\nNow we can create the plot:\n\npheatmap(\n  norm_mat_sel,\n  annotation_col = meta_sort,  # Add the metadata\n  cluster_cols = FALSE,        # Don't cluster samples (=columns, cols)\n  show_rownames = FALSE,       # Don't show gene names\n  scale = \"row\",               # Perform z-scaling for each gene\n  )\n\n\n\n\n\n\n\n\nNotes on the code and plot above:\n\nThe z-scaling with scale = will make sure we can compare genes with very different expression levels: after all, we’re interested in relative expression levels across samples/sample groups.\npheatmap will by default perform hierarchical clustering both at the sample (col) and gene (row) level, such that more similar samples and genes will appear closer to each other. Above, we turned clustering off for samples, since we want to keep them in their by-group order.\n\n\n\n Bonus exercise: heatmaps\nMake a heatmap with the top-25 most-highly expressed genes (i.e., genes with the highest mean expression levels across all samples).\n\n\nClick for a hint: how to get that top-25\n\n\ntop25_hi &lt;- names(sort(rowMeans(norm_mat), decreasing = TRUE)[1:25])\n\n\n\n\nClick for the solution\n\n\n# In the normalized count matrix, select only the genes of interest\nnorm_mat_sel &lt;- norm_mat[match(top25_hi, rownames(norm_mat)), ]\n\n# Sort the metadata\nmeta_sort &lt;- meta |&gt;\n  arrange(treatment, time) |&gt;\n  select(treatment, time)\n\n# Create the heatmap\npheatmap(\n  norm_mat_sel,\n  annotation_col = meta_sort,\n  cluster_cols = FALSE,\n  show_rownames = FALSE,\n  scale = \"row\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.2 NA values in the DESeq2 results table\nSome values in the DESeq2 results table can be set to NA for one of the following reasons:\n\nIf a gene contains a sample with a count outlier, both the p-value and adjusted p-value will be set to NA. (DESeq2 performs outlier detection using Cook’s distance.)\nIf all samples have zero counts for a given gene, the baseMean column will be zero, and the log2-fold change estimates, p-value and adjusted p-value will all be set to NA.\nDESeq2 also automatically filters genes with a low mean count in the sense that it does not include them in the multiple testing correction. Therefore, in such cases, the p-value will not be NA, but the adjusted p-value will be.\nBecause we have very low power to detect differential expression for such low-count genes, it is beneficial to remove them prior to the multiple testing correction: that way, the correction becomes less severe for the remaining genes.\n\nLet’s see how many genes have NA p-values:\n\n# Number of genes with NA p-value:\nsum(is.na(res_rc24$pvalue))\n\n[1] 1124\n\n# As a proportion of the total number of genes in the test:\nsum(is.na(res_rc24$pvalue)) / nrow(res_rc24)\n\n[1] 0.05961283\n\n\nAnd NA adjusted p-values:\n\n# Number of genes with NA p-value:\nsum(is.na(res_rc24$padj))\n\n[1] 7283\n\n# As a proportion of the total number of genes in the test:\nsum(is.na(res_rc24$padj)) / nrow(res_rc24)\n\n[1] 0.3862636\n\n\n\n\n\n8.3 Exporting the results\nTo save the DE results tables, you can for example use the write_tsv() function. You could open the resulting file in Excel for further exploration.\n\n# Create the output directory, if necessary:\ndir.create(\"results/DE\", recursive = TRUE, showWarnings = FALSE)\n\n# Write the \nwrite_tsv(as.data.frame(res_rc24), \"results/DE/resultsres_rc24.tsv\")"
  },
  {
    "objectID": "lab2.html#footnotes",
    "href": "lab2.html#footnotes",
    "title": "Self-study lab: Gene count table analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn older pipe, which requires loading an R package to work↩︎\nThe new base R pipe that does not require a package↩︎\nAnd even for more basic tasks, it is common to use packages that are preferred over the functionality that is by default available in R, like in the case of plotting.↩︎\nSpecifically, the point is to remove the dependence of the variance in expression level on its mean, among genes↩︎\nI can’t tell from the paper which method they used↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HCS7004-SP24",
    "section": "",
    "text": "Schedule for today\n\n1:50-2:45 pm — Lecture: Pipelines, nf-core, and the rnaseq pipeline (slides)\n3:00-4:50 pm — Lab: Running the nf-core rnaseq pipeline\n\n\n\n\nSelf-study lab\n\nAnalyzing the gene count table – Exploratory and differential expression analysis\n\n\n\n\n\n Back to top"
  }
]